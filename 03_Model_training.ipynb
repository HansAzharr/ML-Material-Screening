{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1azzUDNzTBuNpd-yhiwvllgrWDI_XRE82",
      "authorship_tag": "ABX9TyPkbRQIacxCWCB7i/G7DUmN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HansAzharr/ML-Material-Screening/blob/main/03_Model_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MK4lQZMGTIU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import lightgbm as lgb\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import RandomizedSearchCV, KFold\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "import joblib # For saving and loading models and scaler\n",
        "\n",
        "from sklearn.multioutput import MultiOutputRegressor # import for multi-output handling\n",
        "from sklearn.ensemble import RandomForestRegressor\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load file"
      ],
      "metadata": {
        "id": "W1h8Ld45m2lh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_folder_path = '/content/drive/MyDrive/Colab Notebooks/Masters Research Project/Data/'\n",
        "engineered_features_filename = os.path.join(data_folder_path, \"battery_materials_engineered_features_v2.csv\")\n",
        "\n",
        "# Load the engineered data\n",
        "df_engineered = pd.read_csv(engineered_features_filename)\n",
        "print(f\"Successfully loaded engineered features DataFrame from '{engineered_features_filename}'.\")\n",
        "print(f\"DataFrame shape: {df_engineered.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "koW8ClCXm4VN",
        "outputId": "b3cd2092-466e-4efe-adae-f32bff78d651"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded engineered features DataFrame from '/content/drive/MyDrive/Colab Notebooks/Masters Research Project/Data/battery_materials_engineered_features_v2.csv'.\n",
            "DataFrame shape: (3445, 53)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_engineered.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEVcgemlqnkQ",
        "outputId": "2341fb5f-e3ef-43af-e76b-d58236bba2cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3445 entries, 0 to 3444\n",
            "Data columns (total 53 columns):\n",
            " #   Column                       Non-Null Count  Dtype  \n",
            "---  ------                       --------------  -----  \n",
            " 0   material_id                  3445 non-null   object \n",
            " 1   energy_above_hull            3445 non-null   float64\n",
            " 2   density                      3445 non-null   float64\n",
            " 3   formula_pretty               3445 non-null   object \n",
            " 4   nelements                    3445 non-null   float64\n",
            " 5   volume                       3445 non-null   float64\n",
            " 6   band_gap                     3445 non-null   float64\n",
            " 7   total_magnetization          3445 non-null   float64\n",
            " 8   battery_type                 3445 non-null   object \n",
            " 9   average_voltage              3445 non-null   float64\n",
            " 10  theoretical_capacity_grav    3445 non-null   float64\n",
            " 11  theoretical_capacity_vol     3445 non-null   float64\n",
            " 12  energy_grav                  3445 non-null   float64\n",
            " 13  energy_vol                   3445 non-null   float64\n",
            " 14  num_steps                    3445 non-null   float64\n",
            " 15  max_voltage_step             3445 non-null   float64\n",
            " 16  stability_charge             3445 non-null   float64\n",
            " 17  stability_discharge          3445 non-null   float64\n",
            " 18  e_fermi                      3445 non-null   float64\n",
            " 19  nsites                       3445 non-null   int64  \n",
            " 20  num_unique_elements          3445 non-null   float64\n",
            " 21  avg_atomic_number            3445 non-null   float64\n",
            " 22  min_atomic_number            3445 non-null   float64\n",
            " 23  max_atomic_number            3445 non-null   float64\n",
            " 24  avg_atomic_mass              3445 non-null   float64\n",
            " 25  min_atomic_mass              3445 non-null   float64\n",
            " 26  max_atomic_mass              3445 non-null   float64\n",
            " 27  avg_electronegativity        3445 non-null   float64\n",
            " 28  min_electronegativity        3445 non-null   float64\n",
            " 29  max_electronegativity        3445 non-null   float64\n",
            " 30  avg_covalent_radius          3445 non-null   float64\n",
            " 31  min_covalent_radius          3445 non-null   float64\n",
            " 32  max_covalent_radius          3445 non-null   float64\n",
            " 33  std_electronegativity        3445 non-null   float64\n",
            " 34  ionic_radius_ion             3445 non-null   float64\n",
            " 35  atomic_weight_ion            3445 non-null   float64\n",
            " 36  charge_ion                   3445 non-null   int64  \n",
            " 37  atomic_number_ion            3445 non-null   int64  \n",
            " 38  electronegativity_ion        3445 non-null   float64\n",
            " 39  working_ion_Li               3445 non-null   int64  \n",
            " 40  working_ion_Mg               3445 non-null   int64  \n",
            " 41  working_ion_Na               3445 non-null   int64  \n",
            " 42  crystal_system_Cubic         3445 non-null   int64  \n",
            " 43  crystal_system_Hexagonal     3445 non-null   int64  \n",
            " 44  crystal_system_Monoclinic    3445 non-null   int64  \n",
            " 45  crystal_system_Orthorhombic  3445 non-null   int64  \n",
            " 46  crystal_system_Tetragonal    3445 non-null   int64  \n",
            " 47  crystal_system_Triclinic     3445 non-null   int64  \n",
            " 48  crystal_system_Trigonal      3445 non-null   int64  \n",
            " 49  is_metal_False               3445 non-null   int64  \n",
            " 50  is_metal_True                3445 non-null   int64  \n",
            " 51  theoretical_False            3445 non-null   int64  \n",
            " 52  theoretical_True             3445 non-null   int64  \n",
            "dtypes: float64(33), int64(17), object(3)\n",
            "memory usage: 1.4+ MB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Features (X) and Targets (Y)"
      ],
      "metadata": {
        "id": "jv4kKpwGnLzP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Y_cols = ['theoretical_capacity_grav', 'average_voltage', 'energy_above_hull']\n",
        "\n",
        "excluded_cols = ['material_id', 'formula_pretty', 'battery_type', 'theoretical_capacity_vol', 'stability_charge'] + Y_cols\n",
        "\n",
        "X_cols = [col for col in df_engineered.columns if col not in excluded_cols]\n",
        "\n",
        "X = df_engineered[X_cols]\n",
        "Y = df_engineered[Y_cols]\n",
        "\n",
        "print(f\"\\nDefined X (features) with {X.shape[1]} columns and Y (targets) with {Y.shape[1]} columns.\")\n",
        "print(\"X (features) columns:\", X.columns.tolist())\n",
        "print(\"Y (targets) columns:\", Y.columns.tolist())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHOjqZ7EnMNt",
        "outputId": "c9bc8759-8163-41a2-efaa-7a8472525e72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Defined X (features) with 45 columns and Y (targets) with 3 columns.\n",
            "X (features) columns: ['density', 'nelements', 'volume', 'band_gap', 'total_magnetization', 'energy_grav', 'energy_vol', 'num_steps', 'max_voltage_step', 'stability_discharge', 'e_fermi', 'nsites', 'num_unique_elements', 'avg_atomic_number', 'min_atomic_number', 'max_atomic_number', 'avg_atomic_mass', 'min_atomic_mass', 'max_atomic_mass', 'avg_electronegativity', 'min_electronegativity', 'max_electronegativity', 'avg_covalent_radius', 'min_covalent_radius', 'max_covalent_radius', 'std_electronegativity', 'ionic_radius_ion', 'atomic_weight_ion', 'charge_ion', 'atomic_number_ion', 'electronegativity_ion', 'working_ion_Li', 'working_ion_Mg', 'working_ion_Na', 'crystal_system_Cubic', 'crystal_system_Hexagonal', 'crystal_system_Monoclinic', 'crystal_system_Orthorhombic', 'crystal_system_Tetragonal', 'crystal_system_Triclinic', 'crystal_system_Trigonal', 'is_metal_False', 'is_metal_True', 'theoretical_False', 'theoretical_True']\n",
            "Y (targets) columns: ['theoretical_capacity_grav', 'average_voltage', 'energy_above_hull']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train-Test Split"
      ],
      "metadata": {
        "id": "bdR3f83BnySP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=9)\n",
        "print(f\"X_train shape: {X_train.shape}, Y_train shape: {Y_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}, Y_test shape: {Y_test.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZB9R1-Mynyzn",
        "outputId": "9203b55c-5cfc-4640-e1ff-8892f6298580"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (2756, 45), Y_train shape: (2756, 3)\n",
            "X_test shape: (689, 45), Y_test shape: (689, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Scaling (StandardScaler)"
      ],
      "metadata": {
        "id": "W0OjpzcKoKY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select columns that are float64 or int64 dtypes.\n",
        "numerical_cols_for_scaling = X_train.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
        "\n",
        "# Further filter out columns that are actually binary (0 or 1) from this list\n",
        "binary_cols = [col for col in numerical_cols_for_scaling\n",
        "               if X_train[col].nunique() == 2 and X_train[col].min() == 0 and X_train[col].max() == 1]\n",
        "\n",
        "# The actual columns to scale are numerical, minus the identified binary columns\n",
        "numerical_cols_to_scale = [col for col in numerical_cols_for_scaling if col not in binary_cols]\n",
        "\n",
        "print(f\"\\nIdentified {len(numerical_cols_to_scale)} numerical columns for scaling:\")\n",
        "print(numerical_cols_to_scale)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmXr34dvs0Pz",
        "outputId": "506be073-927f-4272-f109-78513c85dc72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Identified 31 numerical columns for scaling:\n",
            "['density', 'nelements', 'volume', 'band_gap', 'total_magnetization', 'energy_grav', 'energy_vol', 'num_steps', 'max_voltage_step', 'stability_discharge', 'e_fermi', 'nsites', 'num_unique_elements', 'avg_atomic_number', 'min_atomic_number', 'max_atomic_number', 'avg_atomic_mass', 'min_atomic_mass', 'max_atomic_mass', 'avg_electronegativity', 'min_electronegativity', 'max_electronegativity', 'avg_covalent_radius', 'min_covalent_radius', 'max_covalent_radius', 'std_electronegativity', 'ionic_radius_ion', 'atomic_weight_ion', 'charge_ion', 'atomic_number_ion', 'electronegativity_ion']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the StandardScaler\n",
        "scaler_X = StandardScaler()\n",
        "\n",
        "# Fit the scaler on the training data ONLY for the identified numerical columns\n",
        "X_train_scaled = X_train.copy() # Create a copy to store scaled values\n",
        "X_test_scaled = X_test.copy()   # Create a copy to store scaled values\n",
        "\n",
        "if numerical_cols_to_scale: # Only scale if there are columns to scale\n",
        "    X_train_scaled[numerical_cols_to_scale] = scaler_X.fit_transform(X_train[numerical_cols_to_scale])\n",
        "    # Transform testing data using the scaler fitted on training data\n",
        "    X_test_scaled[numerical_cols_to_scale] = scaler_X.transform(X_test[numerical_cols_to_scale])\n",
        "    print(\"Numerical features scaled successfully.\")\n",
        "else:\n",
        "    print(\"No continuous numerical features found for scaling.\")\n",
        "    # If no numerical features, ensure scaled variables are just copies of originals\n",
        "    X_train_scaled = X_train.copy()\n",
        "    X_test_scaled = X_test.copy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msJcIZUXoL5j",
        "outputId": "f75df749-40da-4b80-d349-75e974aa86a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numerical features scaled successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display head of scaled training features (check a few numerical ones)\n",
        "\n",
        "# Dynamically select a few numerical features and some encoded ones for display\n",
        "display_cols = []\n",
        "if numerical_cols_to_scale:\n",
        "    display_cols.extend(numerical_cols_to_scale[:3]) # First 3 numerical\n",
        "# Add a few one-hot encoded columns\n",
        "encoded_cols_example = [col for col in X_train.columns if 'crystal_system_' in col or 'working_ion_' in col or 'is_metal_' in col]\n",
        "display_cols.extend(encoded_cols_example[:3]) # First 3 example encoded\n",
        "\n",
        "# Ensure display_cols are unique and exist in X_train_scaled\n",
        "display_cols = list(set(col for col in display_cols if col in X_train_scaled.columns))\n",
        "if display_cols:\n",
        "    print(X_train_scaled[display_cols].head().to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
        "else:\n",
        "    print(\"No suitable columns to display example scaled data.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nu9BPE01tLNE",
        "outputId": "dbd3c167-8c52-4180-e309-e6dc6b42ca71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| working_ion_Mg   | working_ion_Li   | volume    | working_ion_Na   | nelements   | density   |\n",
            "|:-----------------|:-----------------|:----------|:-----------------|:------------|:----------|\n",
            "| 0                | 1                | -0.404032 | 0                | 0.665793    | -0.169257 |\n",
            "| 1                | 0                | -1.19194  | 0                | -0.656199   | 0.497746  |\n",
            "| 0                | 1                | 1.85852   | 0                | -0.656199   | -0.193005 |\n",
            "| 1                | 0                | -0.894973 | 0                | 0.665793    | 3.36416   |\n",
            "| 0                | 1                | -0.646339 | 0                | -1.97819    | -0.235335 |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adding Interaction features"
      ],
      "metadata": {
        "id": "SYRKpS3hEkUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to add interaction features to both train and test sets\n",
        "def add_interaction_features(df, feature1, feature2, new_feature_name):\n",
        "    df[new_feature_name] = df[feature1] * df[feature2]\n",
        "    return df\n",
        "\n",
        "# --- Selected Interactions with theoretical_False (Experimental) ---\n",
        "X_train_scaled = add_interaction_features(X_train_scaled, 'theoretical_False', 'band_gap', 'exp_band_gap')\n",
        "X_test_scaled = add_interaction_features(X_test_scaled, 'theoretical_False', 'band_gap', 'exp_band_gap')\n",
        "\n",
        "X_train_scaled = add_interaction_features(X_train_scaled, 'theoretical_False', 'energy_vol', 'exp_energy_vol')\n",
        "X_test_scaled = add_interaction_features(X_test_scaled, 'theoretical_False', 'energy_vol', 'exp_energy_vol')\n",
        "\n",
        "# --- Selected Interactions with theoretical_True (Theoretical) ---\n",
        "X_train_scaled = add_interaction_features(X_train_scaled, 'theoretical_True', 'band_gap', 'theo_band_gap')\n",
        "X_test_scaled = add_interaction_features(X_test_scaled, 'theoretical_True', 'band_gap', 'theo_band_gap')\n",
        "\n",
        "X_train_scaled = add_interaction_features(X_train_scaled, 'theoretical_True', 'energy_vol', 'theo_energy_vol')\n",
        "X_test_scaled = add_interaction_features(X_test_scaled, 'theoretical_True', 'energy_vol', 'theo_energy_vol')\n",
        "\n",
        "print(\"Selected 4 new features added to X_train_scaled and X_test_scaled.\")\n",
        "print(f\"New total number of features in X_train_scaled: {X_train_scaled.shape[1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjkUDpfbEoc0",
        "outputId": "ef3823c5-bed5-49d0-a7fd-171b38ed7793"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected 4 new features added to X_train_scaled and X_test_scaled.\n",
            "New total number of features in X_train_scaled: 49\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_scaled.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6K-7JOa5uEU",
        "outputId": "39a324fe-433d-4b38-e961-61f69fcd3a79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 2756 entries, 2553 to 382\n",
            "Data columns (total 49 columns):\n",
            " #   Column                       Non-Null Count  Dtype  \n",
            "---  ------                       --------------  -----  \n",
            " 0   density                      2756 non-null   float64\n",
            " 1   nelements                    2756 non-null   float64\n",
            " 2   volume                       2756 non-null   float64\n",
            " 3   band_gap                     2756 non-null   float64\n",
            " 4   total_magnetization          2756 non-null   float64\n",
            " 5   energy_grav                  2756 non-null   float64\n",
            " 6   energy_vol                   2756 non-null   float64\n",
            " 7   num_steps                    2756 non-null   float64\n",
            " 8   max_voltage_step             2756 non-null   float64\n",
            " 9   stability_discharge          2756 non-null   float64\n",
            " 10  e_fermi                      2756 non-null   float64\n",
            " 11  nsites                       2756 non-null   float64\n",
            " 12  num_unique_elements          2756 non-null   float64\n",
            " 13  avg_atomic_number            2756 non-null   float64\n",
            " 14  min_atomic_number            2756 non-null   float64\n",
            " 15  max_atomic_number            2756 non-null   float64\n",
            " 16  avg_atomic_mass              2756 non-null   float64\n",
            " 17  min_atomic_mass              2756 non-null   float64\n",
            " 18  max_atomic_mass              2756 non-null   float64\n",
            " 19  avg_electronegativity        2756 non-null   float64\n",
            " 20  min_electronegativity        2756 non-null   float64\n",
            " 21  max_electronegativity        2756 non-null   float64\n",
            " 22  avg_covalent_radius          2756 non-null   float64\n",
            " 23  min_covalent_radius          2756 non-null   float64\n",
            " 24  max_covalent_radius          2756 non-null   float64\n",
            " 25  std_electronegativity        2756 non-null   float64\n",
            " 26  ionic_radius_ion             2756 non-null   float64\n",
            " 27  atomic_weight_ion            2756 non-null   float64\n",
            " 28  charge_ion                   2756 non-null   float64\n",
            " 29  atomic_number_ion            2756 non-null   float64\n",
            " 30  electronegativity_ion        2756 non-null   float64\n",
            " 31  working_ion_Li               2756 non-null   int64  \n",
            " 32  working_ion_Mg               2756 non-null   int64  \n",
            " 33  working_ion_Na               2756 non-null   int64  \n",
            " 34  crystal_system_Cubic         2756 non-null   int64  \n",
            " 35  crystal_system_Hexagonal     2756 non-null   int64  \n",
            " 36  crystal_system_Monoclinic    2756 non-null   int64  \n",
            " 37  crystal_system_Orthorhombic  2756 non-null   int64  \n",
            " 38  crystal_system_Tetragonal    2756 non-null   int64  \n",
            " 39  crystal_system_Triclinic     2756 non-null   int64  \n",
            " 40  crystal_system_Trigonal      2756 non-null   int64  \n",
            " 41  is_metal_False               2756 non-null   int64  \n",
            " 42  is_metal_True                2756 non-null   int64  \n",
            " 43  theoretical_False            2756 non-null   int64  \n",
            " 44  theoretical_True             2756 non-null   int64  \n",
            " 45  exp_band_gap                 2756 non-null   float64\n",
            " 46  exp_energy_vol               2756 non-null   float64\n",
            " 47  theo_band_gap                2756 non-null   float64\n",
            " 48  theo_energy_vol              2756 non-null   float64\n",
            "dtypes: float64(35), int64(14)\n",
            "memory usage: 1.1 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XGBoost"
      ],
      "metadata": {
        "id": "oqCvggAByjUb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb"
      ],
      "metadata": {
        "id": "yX4OVIRMyk_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the base XGBoost model\n",
        "\n",
        "xgb_base_model = xgb.XGBRegressor(\n",
        "    objective='reg:squarederror',\n",
        "    n_estimators=300,  # A good starting point for number of boosting rounds\n",
        "    learning_rate=0.1, # Step size shrinkage\n",
        "    max_depth=6,       # Maximum depth of a tree\n",
        "    subsample=0.8,     # Subsample ratio of the training instance\n",
        "    colsample_bytree=0.8, # Subsample ratio of columns when constructing each tree\n",
        "    random_state=9,\n",
        "    n_jobs=-1\n",
        ")"
      ],
      "metadata": {
        "id": "LPokAq-Sylee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrap the XGBoost model in MultiOutputRegressor\n",
        "multioutput_xgb = MultiOutputRegressor(xgb_base_model)\n",
        "\n",
        "# Prepare lists to store scores for each target across all folds\n",
        "xgb_target_r2_scores = {col: [] for col in Y_train.columns}\n",
        "xgb_target_mae_scores = {col: [] for col in Y_train.columns}\n",
        "xgb_target_rmse_scores = {col: [] for col in Y_train.columns}"
      ],
      "metadata": {
        "id": "50oZ118rypSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform cross-validation with the XGBoost model\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=9)\n",
        "\n",
        "for fold_idx, (train_index, test_index) in enumerate(kfold.split(X_train_scaled, Y_train)):\n",
        "    X_train_fold, X_test_fold = X_train_scaled.iloc[train_index], X_train_scaled.iloc[test_index]\n",
        "    Y_train_fold, Y_test_fold = Y_train.iloc[train_index], Y_train.iloc[test_index]\n",
        "\n",
        "    # Fit a new instance of the MultiOutput XGBoost model for each fold\n",
        "    # This automatically trains a separate XGBoost model for each target\n",
        "    fold_multioutput_xgb = MultiOutputRegressor(xgb_base_model)\n",
        "    fold_multioutput_xgb.fit(X_train_fold, Y_train_fold)\n",
        "\n",
        "    Y_pred_fold = fold_multioutput_xgb.predict(X_test_fold)\n",
        "    # Convert predictions back to DataFrame with column names for clarity\n",
        "    Y_pred_fold_df = pd.DataFrame(Y_pred_fold, columns=Y_test_fold.columns, index=Y_test_fold.index)\n",
        "\n",
        "    for i, target_col in enumerate(Y_test_fold.columns):\n",
        "        r2 = r2_score(Y_test_fold[target_col], Y_pred_fold_df[target_col])\n",
        "        mae = mean_absolute_error(Y_test_fold[target_col], Y_pred_fold_df[target_col])\n",
        "        rmse = np.sqrt(mean_squared_error(Y_test_fold[target_col], Y_pred_fold_df[target_col]))\n",
        "\n",
        "        xgb_target_r2_scores[target_col].append(r2)\n",
        "        xgb_target_mae_scores[target_col].append(mae)\n",
        "        xgb_target_rmse_scores[target_col].append(rmse)"
      ],
      "metadata": {
        "id": "IUJKrSI_yscq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print aggregated cross-validation results for XGBoost\n",
        "print(\"\\n--- XGBoost Cross-Validation Results (Mean across 5 folds, Per Target) ---\")\n",
        "for target_col in Y_train.columns:\n",
        "    print(f\"\\nMetrics for Target: {target_col}\")\n",
        "    print(f\"  Mean R-squared (R²): {np.mean(xgb_target_r2_scores[target_col]):.4f} (Std Dev: {np.std(xgb_target_r2_scores[target_col]):.4f})\")\n",
        "    print(f\"  Mean MAE: {np.mean(xgb_target_mae_scores[target_col]):.4f} (Std Dev: {np.std(xgb_target_mae_scores[target_col]):.4f})\")\n",
        "    print(f\"  Mean RMSE: {np.mean(xgb_target_rmse_scores[target_col]):.4f} (Std Dev: {np.std(xgb_target_rmse_scores[target_col]):.4f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBf_er_Wyw5y",
        "outputId": "f2249bdc-1813-47f7-ec58-499246f5a7c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- XGBoost Cross-Validation Results (Mean across 5 folds, Per Target) ---\n",
            "\n",
            "Metrics for Target: theoretical_capacity_grav\n",
            "  Mean R-squared (R²): 0.8422 (Std Dev: 0.0444)\n",
            "  Mean MAE: 20.7343 (Std Dev: 0.9978)\n",
            "  Mean RMSE: 56.5900 (Std Dev: 11.3620)\n",
            "\n",
            "Metrics for Target: average_voltage\n",
            "  Mean R-squared (R²): 0.8030 (Std Dev: 0.1680)\n",
            "  Mean MAE: 0.4322 (Std Dev: 0.1859)\n",
            "  Mean RMSE: 2.8554 (Std Dev: 3.7643)\n",
            "\n",
            "Metrics for Target: energy_above_hull\n",
            "  Mean R-squared (R²): 0.4669 (Std Dev: 0.0264)\n",
            "  Mean MAE: 0.0244 (Std Dev: 0.0007)\n",
            "  Mean RMSE: 0.0331 (Std Dev: 0.0008)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train on the full training set and evaluate on test set for a final view\n",
        "\n",
        "print(\"\\n--- XGBoost Model Evaluation on Test Set ---\")\n",
        "multioutput_xgb_final = MultiOutputRegressor(xgb_base_model)\n",
        "multioutput_xgb_final.fit(X_train_scaled, Y_train) # Train on full training data\n",
        "\n",
        "Y_pred_xgb_test = multioutput_xgb_final.predict(X_test_scaled)\n",
        "Y_pred_xgb_test_df = pd.DataFrame(Y_pred_xgb_test, columns=Y_test.columns, index=Y_test.index)\n",
        "\n",
        "for i, target_col in enumerate(Y_test.columns):\n",
        "    r2 = r2_score(Y_test[target_col], Y_pred_xgb_test_df[target_col])\n",
        "    mae = mean_absolute_error(Y_test[target_col], Y_pred_xgb_test_df[target_col])\n",
        "    rmse = np.sqrt(mean_squared_error(Y_test[target_col], Y_pred_xgb_test_df[target_col]))\n",
        "\n",
        "    print(f\"\\nMetrics for Target: {target_col} (Test Set)\")\n",
        "    print(f\"  R-squared (R²): {r2:.4f}\")\n",
        "    print(f\"  Mean Absolute Error (MAE): {mae:.4f}\")\n",
        "    print(f\"  Root Mean Squared Error (RMSE): {rmse:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8AgUweDyz_g",
        "outputId": "5b2a8ac1-7e7b-4adb-827b-29b04337b34b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- XGBoost Model Evaluation on Test Set ---\n",
            "\n",
            "Metrics for Target: theoretical_capacity_grav (Test Set)\n",
            "  R-squared (R²): 0.7347\n",
            "  Mean Absolute Error (MAE): 17.4324\n",
            "  Root Mean Squared Error (RMSE): 36.7951\n",
            "\n",
            "Metrics for Target: average_voltage (Test Set)\n",
            "  R-squared (R²): 0.8565\n",
            "  Mean Absolute Error (MAE): 0.3703\n",
            "  Root Mean Squared Error (RMSE): 1.5015\n",
            "\n",
            "Metrics for Target: energy_above_hull (Test Set)\n",
            "  R-squared (R²): 0.5203\n",
            "  Mean Absolute Error (MAE): 0.0232\n",
            "  Root Mean Squared Error (RMSE): 0.0319\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing on dataset"
      ],
      "metadata": {
        "id": "W4nybngjKAQ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random"
      ],
      "metadata": {
        "id": "WNsaNPMnKDbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select a random index from the test set\n",
        "random_index = random.randint(0, len(X_test_scaled) - 1)\n",
        "\n",
        "# Get the actual features and targets for the random material\n",
        "random_material_features = X_test_scaled.iloc[[random_index]]\n",
        "random_material_actual_targets = Y_test.iloc[[random_index]]\n",
        "\n",
        "# Get the original material id and formula\n",
        "original_material_id = df_engineered.loc[random_material_features.index[0], 'material_id']\n",
        "original_formula_pretty = df_engineered.loc[random_material_features.index[0], 'formula_pretty']\n",
        "\n",
        "# Predict the targets for the random material\n",
        "random_material_predicted_targets_scaled = multioutput_xgb_final.predict(random_material_features)\n",
        "\n",
        "# Since the model was trained on scaled features, the predictions are directly comparable\n",
        "# to the scaled Y values if Y was scaled. However, Y was NOT scaled in this notebook.\n",
        "# So, the predictions are in the original scale of the target variables.\n",
        "\n",
        "print(f\"\\nTesting prediction on a random material (Index: {random_index}):\")\n",
        "print(f\"  Material ID: {original_material_id}\")\n",
        "print(f\"  Formula: {original_formula_pretty}\")\n",
        "\n",
        "print(\"\\nActual Target Values:\")\n",
        "# Add units to the column names for actual targets\n",
        "actual_targets_with_units = random_material_actual_targets.copy()\n",
        "actual_targets_with_units.columns = ['theoretical_capacity_grav (mAh/g)', 'average_voltage (V)', 'energy_above_hull (eV/atom)']\n",
        "print(actual_targets_with_units.to_markdown(numalign=\"left\", stralign=\"left\"))\n",
        "\n",
        "\n",
        "print(\"\\nPredicted Target Values:\")\n",
        "# Convert the numpy array prediction to a pandas DataFrame for easier printing\n",
        "predicted_df = pd.DataFrame(random_material_predicted_targets_scaled, columns=Y_cols, index=[0])\n",
        "# Add units to the column names for predicted targets\n",
        "predicted_df.columns = ['theoretical_capacity_grav (mAh/g)', 'average_voltage (V)', 'energy_above_hull (eV/atom)']\n",
        "print(predicted_df.to_markdown(numalign=\"left\", stralign=\"left\"))\n",
        "\n",
        "# Calculate absolute errors for this specific material\n",
        "abs_errors = np.abs(random_material_actual_targets.values - random_material_predicted_targets_scaled)\n",
        "# Add units to the column names for absolute errors\n",
        "abs_errors_df = pd.DataFrame(abs_errors, columns=[f'Absolute Error ({col})' for col in ['theoretical_capacity_grav (mAh/g)', 'average_voltage (V)', 'energy_above_hull (eV/atom)']], index=[0])\n",
        "\n",
        "print(\"\\nAbsolute Errors for this Prediction:\")\n",
        "print(abs_errors_df.to_markdown(numalign=\"left\", stralign=\"left\"))\n",
        "\n",
        "# Calculate relative errors for this specific material\n",
        "# Avoid division by zero for actual values close to zero\n",
        "relative_errors = np.where(random_material_actual_targets.values != 0,\n",
        "                           abs_errors / random_material_actual_targets.values,\n",
        "                           np.nan) # Use NaN where actual value is 0\n",
        "\n",
        "# Add units to the column names for relative errors\n",
        "relative_errors_df = pd.DataFrame(relative_errors, columns=[f'Relative Error ({col})' for col in ['theoretical_capacity_grav (%)', 'average_voltage (%)', 'energy_above_hull (%)']], index=[0])\n",
        "\n",
        "print(\"\\nRelative Errors for this Prediction (as %):\")\n",
        "# Convert relative error to percentage for display\n",
        "relative_errors_df = relative_errors_df * 100\n",
        "print(relative_errors_df.to_markdown(numalign=\"left\", stralign=\"left\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fR2VDfQ5J__I",
        "outputId": "191d0544-681b-404d-b391-396d94c2fd57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing prediction on a random material (Index: 539):\n",
            "  Material ID: mp-758020\n",
            "  Formula: LiCu3(CO3)3\n",
            "\n",
            "Actual Target Values:\n",
            "|      | theoretical_capacity_grav (mAh/g)   | average_voltage (V)   | energy_above_hull (eV/atom)   |\n",
            "|:-----|:------------------------------------|:----------------------|:------------------------------|\n",
            "| 2210 | 136.921                             | 2.44149               | 0.110563                      |\n",
            "\n",
            "Predicted Target Values:\n",
            "|    | theoretical_capacity_grav (mAh/g)   | average_voltage (V)   | energy_above_hull (eV/atom)   |\n",
            "|:---|:------------------------------------|:----------------------|:------------------------------|\n",
            "| 0  | 154.749                             | 2.35688               | 0.0890837                     |\n",
            "\n",
            "Absolute Errors for this Prediction:\n",
            "|    | Absolute Error (theoretical_capacity_grav (mAh/g))   | Absolute Error (average_voltage (V))   | Absolute Error (energy_above_hull (eV/atom))   |\n",
            "|:---|:-----------------------------------------------------|:---------------------------------------|:-----------------------------------------------|\n",
            "| 0  | 17.8276                                              | 0.0846087                              | 0.0214792                                      |\n",
            "\n",
            "Relative Errors for this Prediction (as %):\n",
            "|    | Relative Error (theoretical_capacity_grav (%))   | Relative Error (average_voltage (%))   | Relative Error (energy_above_hull (%))   |\n",
            "|:---|:-------------------------------------------------|:---------------------------------------|:-----------------------------------------|\n",
            "| 0  | 13.0203                                          | 3.46546                                | 19.4272                                  |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter Tuning for XGBoost Model"
      ],
      "metadata": {
        "id": "gGJOTkH8zvNY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the base XGBoost model\n",
        "xgb_base_model_tuned = xgb.XGBRegressor(\n",
        "    objective='reg:squarederror', # Standard for regression\n",
        "    random_state=9,\n",
        "    n_jobs=-1, # Use all available CPU cores\n",
        "    tree_method='hist' # Often faster for larger datasets\n",
        ")"
      ],
      "metadata": {
        "id": "tNnPNHXnzwPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrap it in MultiOutputRegressor\n",
        "multioutput_xgb_tuned = MultiOutputRegressor(xgb_base_model_tuned)\n",
        "\n",
        "# Define the parameter distribution for RandomizedSearchCV for XGBoost\n",
        "param_distributions_xgb = {\n",
        "    'estimator__n_estimators': [100, 200, 300, 400, 500, 600], # Number of boosting rounds (trees)\n",
        "    'estimator__learning_rate': [0.01, 0.05, 0.1, 0.15, 0.2], # Step size shrinkage\n",
        "    'estimator__max_depth': [3, 4, 5, 6, 7, 8, 9, 10], # Maximum depth of a tree\n",
        "    'estimator__subsample': [0.6, 0.7, 0.8, 0.9, 1.0], # Subsample ratio of the training instance\n",
        "    'estimator__colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0], # Subsample ratio of columns when constructing each tree\n",
        "    'estimator__gamma': [0, 0.1, 0.2, 0.3, 0.4], # Minimum loss reduction required to make a further partition on a leaf node\n",
        "    'estimator__reg_alpha': [0, 0.005, 0.01, 0.05, 0.1, 0.5, 1], # L1 regularization term on weights\n",
        "    'estimator__reg_lambda': [0, 0.005, 0.01, 0.05, 0.1, 0.5, 1] # L2 regularization term on weights\n",
        "}\n"
      ],
      "metadata": {
        "id": "EFL6HlZEzz8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize RandomizedSearchCV\n",
        "random_search_xgb = RandomizedSearchCV(\n",
        "    estimator=multioutput_xgb_tuned,\n",
        "    param_distributions=param_distributions_xgb,\n",
        "    n_iter=20, # Increased n_iter for a more thorough search\n",
        "    cv=5,\n",
        "    verbose=2, # Set to 1 or 2 for more detailed output during search\n",
        "    random_state=9,\n",
        "    n_jobs=-1 # Use all available CPU cores\n",
        ")\n",
        "\n",
        "# Fit RandomizedSearchCV to the scaled training data and targets\n",
        "random_search_xgb.fit(X_train_scaled, Y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "EFr-2WJSz60-",
        "outputId": "22a10c4b-e7bd-437d-ea7d-18e877fc0ad6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=5,\n",
              "                   estimator=MultiOutputRegressor(estimator=XGBRegressor(base_score=None,\n",
              "                                                                         booster=None,\n",
              "                                                                         callbacks=None,\n",
              "                                                                         colsample_bylevel=None,\n",
              "                                                                         colsample_bynode=None,\n",
              "                                                                         colsample_bytree=None,\n",
              "                                                                         device=None,\n",
              "                                                                         early_stopping_rounds=None,\n",
              "                                                                         enable_categorical=False,\n",
              "                                                                         eval_metric=None,\n",
              "                                                                         feature_types=None,\n",
              "                                                                         gamma=None,\n",
              "                                                                         grow_policy=None,\n",
              "                                                                         importance_type=None,\n",
              "                                                                         interaction_...\n",
              "                                        'estimator__gamma': [0, 0.1, 0.2, 0.3,\n",
              "                                                             0.4],\n",
              "                                        'estimator__learning_rate': [0.01, 0.05,\n",
              "                                                                     0.1, 0.15,\n",
              "                                                                     0.2],\n",
              "                                        'estimator__max_depth': [3, 4, 5, 6, 7,\n",
              "                                                                 8, 9, 10],\n",
              "                                        'estimator__n_estimators': [100, 200,\n",
              "                                                                    300, 400,\n",
              "                                                                    500, 600],\n",
              "                                        'estimator__reg_alpha': [0, 0.005, 0.01,\n",
              "                                                                 0.05, 0.1, 0.5,\n",
              "                                                                 1],\n",
              "                                        'estimator__reg_lambda': [0, 0.005,\n",
              "                                                                  0.01, 0.05,\n",
              "                                                                  0.1, 0.5, 1],\n",
              "                                        'estimator__subsample': [0.6, 0.7, 0.8,\n",
              "                                                                 0.9, 1.0]},\n",
              "                   random_state=9, verbose=2)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: #000;\n",
              "  --sklearn-color-text-muted: #666;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-1 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-1 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: flex;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "  align-items: start;\n",
              "  justify-content: space-between;\n",
              "  gap: 0.5em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
              "  font-size: 0.6rem;\n",
              "  font-weight: lighter;\n",
              "  color: var(--sklearn-color-text-muted);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-1 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-1 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 0.5em;\n",
              "  text-align: center;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-1 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=5,\n",
              "                   estimator=MultiOutputRegressor(estimator=XGBRegressor(base_score=None,\n",
              "                                                                         booster=None,\n",
              "                                                                         callbacks=None,\n",
              "                                                                         colsample_bylevel=None,\n",
              "                                                                         colsample_bynode=None,\n",
              "                                                                         colsample_bytree=None,\n",
              "                                                                         device=None,\n",
              "                                                                         early_stopping_rounds=None,\n",
              "                                                                         enable_categorical=False,\n",
              "                                                                         eval_metric=None,\n",
              "                                                                         feature_types=None,\n",
              "                                                                         gamma=None,\n",
              "                                                                         grow_policy=None,\n",
              "                                                                         importance_type=None,\n",
              "                                                                         interaction_...\n",
              "                                        &#x27;estimator__gamma&#x27;: [0, 0.1, 0.2, 0.3,\n",
              "                                                             0.4],\n",
              "                                        &#x27;estimator__learning_rate&#x27;: [0.01, 0.05,\n",
              "                                                                     0.1, 0.15,\n",
              "                                                                     0.2],\n",
              "                                        &#x27;estimator__max_depth&#x27;: [3, 4, 5, 6, 7,\n",
              "                                                                 8, 9, 10],\n",
              "                                        &#x27;estimator__n_estimators&#x27;: [100, 200,\n",
              "                                                                    300, 400,\n",
              "                                                                    500, 600],\n",
              "                                        &#x27;estimator__reg_alpha&#x27;: [0, 0.005, 0.01,\n",
              "                                                                 0.05, 0.1, 0.5,\n",
              "                                                                 1],\n",
              "                                        &#x27;estimator__reg_lambda&#x27;: [0, 0.005,\n",
              "                                                                  0.01, 0.05,\n",
              "                                                                  0.1, 0.5, 1],\n",
              "                                        &#x27;estimator__subsample&#x27;: [0.6, 0.7, 0.8,\n",
              "                                                                 0.9, 1.0]},\n",
              "                   random_state=9, verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>RandomizedSearchCV</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.model_selection.RandomizedSearchCV.html\">?<span>Documentation for RandomizedSearchCV</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>RandomizedSearchCV(cv=5,\n",
              "                   estimator=MultiOutputRegressor(estimator=XGBRegressor(base_score=None,\n",
              "                                                                         booster=None,\n",
              "                                                                         callbacks=None,\n",
              "                                                                         colsample_bylevel=None,\n",
              "                                                                         colsample_bynode=None,\n",
              "                                                                         colsample_bytree=None,\n",
              "                                                                         device=None,\n",
              "                                                                         early_stopping_rounds=None,\n",
              "                                                                         enable_categorical=False,\n",
              "                                                                         eval_metric=None,\n",
              "                                                                         feature_types=None,\n",
              "                                                                         gamma=None,\n",
              "                                                                         grow_policy=None,\n",
              "                                                                         importance_type=None,\n",
              "                                                                         interaction_...\n",
              "                                        &#x27;estimator__gamma&#x27;: [0, 0.1, 0.2, 0.3,\n",
              "                                                             0.4],\n",
              "                                        &#x27;estimator__learning_rate&#x27;: [0.01, 0.05,\n",
              "                                                                     0.1, 0.15,\n",
              "                                                                     0.2],\n",
              "                                        &#x27;estimator__max_depth&#x27;: [3, 4, 5, 6, 7,\n",
              "                                                                 8, 9, 10],\n",
              "                                        &#x27;estimator__n_estimators&#x27;: [100, 200,\n",
              "                                                                    300, 400,\n",
              "                                                                    500, 600],\n",
              "                                        &#x27;estimator__reg_alpha&#x27;: [0, 0.005, 0.01,\n",
              "                                                                 0.05, 0.1, 0.5,\n",
              "                                                                 1],\n",
              "                                        &#x27;estimator__reg_lambda&#x27;: [0, 0.005,\n",
              "                                                                  0.01, 0.05,\n",
              "                                                                  0.1, 0.5, 1],\n",
              "                                        &#x27;estimator__subsample&#x27;: [0.6, 0.7, 0.8,\n",
              "                                                                 0.9, 1.0]},\n",
              "                   random_state=9, verbose=2)</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>best_estimator_: MultiOutputRegressor</div></div></label><div class=\"sk-toggleable__content fitted\"><pre>MultiOutputRegressor(estimator=XGBRegressor(base_score=None, booster=None,\n",
              "                                            callbacks=None,\n",
              "                                            colsample_bylevel=None,\n",
              "                                            colsample_bynode=None,\n",
              "                                            colsample_bytree=1.0, device=None,\n",
              "                                            early_stopping_rounds=None,\n",
              "                                            enable_categorical=False,\n",
              "                                            eval_metric=None,\n",
              "                                            feature_types=None, gamma=0,\n",
              "                                            grow_policy=None,\n",
              "                                            importance_type=None,\n",
              "                                            interaction_constraints=None,\n",
              "                                            learning_rate=0.15, max_bin=None,\n",
              "                                            max_cat_threshold=None,\n",
              "                                            max_cat_to_onehot=None,\n",
              "                                            max_delta_step=None, max_depth=8,\n",
              "                                            max_leaves=None,\n",
              "                                            min_child_weight=None, missing=nan,\n",
              "                                            monotone_constraints=None,\n",
              "                                            multi_strategy=None,\n",
              "                                            n_estimators=200, n_jobs=-1,\n",
              "                                            num_parallel_tree=None,\n",
              "                                            random_state=9, ...))</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>estimator: XGBRegressor</div></div></label><div class=\"sk-toggleable__content fitted\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
              "             colsample_bylevel=None, colsample_bynode=None,\n",
              "             colsample_bytree=1.0, device=None, early_stopping_rounds=None,\n",
              "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
              "             gamma=0, grow_policy=None, importance_type=None,\n",
              "             interaction_constraints=None, learning_rate=0.15, max_bin=None,\n",
              "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
              "             max_delta_step=None, max_depth=8, max_leaves=None,\n",
              "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
              "             multi_strategy=None, n_estimators=200, n_jobs=-1,\n",
              "             num_parallel_tree=None, random_state=9, ...)</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>XGBRegressor</div></div></label><div class=\"sk-toggleable__content fitted\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
              "             colsample_bylevel=None, colsample_bynode=None,\n",
              "             colsample_bytree=1.0, device=None, early_stopping_rounds=None,\n",
              "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
              "             gamma=0, grow_policy=None, importance_type=None,\n",
              "             interaction_constraints=None, learning_rate=0.15, max_bin=None,\n",
              "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
              "             max_delta_step=None, max_depth=8, max_leaves=None,\n",
              "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
              "             multi_strategy=None, n_estimators=200, n_jobs=-1,\n",
              "             num_parallel_tree=None, random_state=9, ...)</pre></div> </div></div></div></div></div></div></div></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the best parameters and the best score\n",
        "best_params_xgb = random_search_xgb.best_params_\n",
        "best_score_xgb = random_search_xgb.best_score_\n",
        "\n",
        "print(f\"Best parameters found for XGBoost: {best_params_xgb}\")\n",
        "print(f\"Best score found for XGBoost: {best_score_xgb}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biKqAUxZ0KzK",
        "outputId": "c5fddb29-5cea-4a93-ea59-31c2b49938ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters found for XGBoost: {'estimator__subsample': 0.8, 'estimator__reg_lambda': 1, 'estimator__reg_alpha': 0, 'estimator__n_estimators': 200, 'estimator__max_depth': 8, 'estimator__learning_rate': 0.15, 'estimator__gamma': 0, 'estimator__colsample_bytree': 1.0}\n",
            "Best score found for XGBoost: 0.6646795511245728\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-evaluate the best model with cross-validation on all targets for robust metrics\n",
        "\n",
        "# Get the best estimator from the search\n",
        "best_multioutput_xgb = random_search_xgb.best_estimator_\n",
        "\n",
        "# Prepare lists to store scores for each target across all folds\n",
        "xgb_tuned_target_r2_scores = {col: [] for col in Y_train.columns}\n",
        "xgb_tuned_target_mae_scores = {col: [] for col in Y_train.columns}\n",
        "xgb_tuned_target_rmse_scores = {col: [] for col in Y_train.columns}\n"
      ],
      "metadata": {
        "id": "z0v15wcv0O2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform cross-validation with the best estimator\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=9)\n",
        "\n",
        "for fold_idx, (train_index, test_index) in enumerate(kfold.split(X_train_scaled, Y_train)):\n",
        "    X_train_fold, X_test_fold = X_train_scaled.iloc[train_index], X_train_scaled.iloc[test_index]\n",
        "    Y_train_fold, Y_test_fold = Y_train.iloc[train_index], Y_train.iloc[test_index]\n",
        "\n",
        "    # Fit a new instance of the best model for each fold (important for proper CV)\n",
        "    # Extract the base estimator parameters from best_params_xgb\n",
        "    fold_xgb_base_model_tuned = xgb.XGBRegressor(\n",
        "        objective='reg:squarederror',\n",
        "        n_estimators=best_params_xgb['estimator__n_estimators'],\n",
        "        learning_rate=best_params_xgb['estimator__learning_rate'],\n",
        "        max_depth=best_params_xgb['estimator__max_depth'],\n",
        "        subsample=best_params_xgb['estimator__subsample'],\n",
        "        colsample_bytree=best_params_xgb['estimator__colsample_bytree'],\n",
        "        gamma=best_params_xgb['estimator__gamma'],\n",
        "        reg_alpha=best_params_xgb['estimator__reg_alpha'],\n",
        "        reg_lambda=best_params_xgb['estimator__reg_lambda'],\n",
        "        random_state=9,\n",
        "        n_jobs=-1,\n",
        "        tree_method='hist'\n",
        "    )\n",
        "    fold_multioutput_xgb_tuned = MultiOutputRegressor(fold_xgb_base_model_tuned)\n",
        "    fold_multioutput_xgb_tuned.fit(X_train_fold, Y_train_fold)\n",
        "\n",
        "    Y_pred_fold = fold_multioutput_xgb_tuned.predict(X_test_fold)\n",
        "    Y_pred_fold_df = pd.DataFrame(Y_pred_fold, columns=Y_test_fold.columns, index=Y_test_fold.index)\n",
        "\n",
        "    for i, target_col in enumerate(Y_test_fold.columns):\n",
        "        r2 = r2_score(Y_test_fold[target_col], Y_pred_fold_df[target_col])\n",
        "        mae = mean_absolute_error(Y_test_fold[target_col], Y_pred_fold_df[target_col])\n",
        "        rmse = np.sqrt(mean_squared_error(Y_test_fold[target_col], Y_pred_fold_df[target_col]))\n",
        "\n",
        "        xgb_tuned_target_r2_scores[target_col].append(r2)\n",
        "        xgb_tuned_target_mae_scores[target_col].append(mae)\n",
        "        xgb_tuned_target_rmse_scores[target_col].append(rmse)"
      ],
      "metadata": {
        "id": "DCEeySHh0Ttc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print aggregated cross-validation results for the tuned XGBoost\n",
        "print(\"\\n--- Tuned XGBoost Cross-Validation Results (Mean across 5 folds, Per Target) ---\")\n",
        "for target_col in Y_train.columns:\n",
        "    print(f\"\\nMetrics for Target: {target_col}\")\n",
        "    print(f\"  Mean R-squared (R²): {np.mean(xgb_tuned_target_r2_scores[target_col]):.4f} (Std Dev: {np.std(xgb_tuned_target_r2_scores[target_col]):.4f})\")\n",
        "    print(f\"  Mean MAE: {np.mean(xgb_tuned_target_mae_scores[target_col]):.4f} (Std Dev: {np.std(xgb_tuned_target_mae_scores[target_col]):.4f})\")\n",
        "    print(f\"  Mean RMSE: {np.mean(xgb_tuned_target_rmse_scores[target_col]):.4f} (Std Dev: {np.std(xgb_tuned_target_rmse_scores[target_col]):.4f})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inwtVaB00V9f",
        "outputId": "c636e20e-4fbf-488d-8783-8f8eea0586b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Tuned XGBoost Cross-Validation Results (Mean across 5 folds, Per Target) ---\n",
            "\n",
            "Metrics for Target: theoretical_capacity_grav\n",
            "  Mean R-squared (R²): 0.8705 (Std Dev: 0.0248)\n",
            "  Mean MAE: 19.9334 (Std Dev: 1.4438)\n",
            "  Mean RMSE: 52.2823 (Std Dev: 12.1327)\n",
            "\n",
            "Metrics for Target: average_voltage\n",
            "  Mean R-squared (R²): 0.8067 (Std Dev: 0.1540)\n",
            "  Mean MAE: 0.4362 (Std Dev: 0.1741)\n",
            "  Mean RMSE: 2.8908 (Std Dev: 3.6284)\n",
            "\n",
            "Metrics for Target: energy_above_hull\n",
            "  Mean R-squared (R²): 0.4592 (Std Dev: 0.0350)\n",
            "  Mean MAE: 0.0246 (Std Dev: 0.0009)\n",
            "  Mean RMSE: 0.0333 (Std Dev: 0.0011)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Final Evaluation on the untouched Test Set with the best tuned model\n",
        "print(\"\\n--- Final Evaluation of Tuned XGBoost Model on Test Set ---\")\n",
        "Y_pred_tuned_xgb_test = best_multioutput_xgb.predict(X_test_scaled)\n",
        "Y_pred_tuned_xgb_test_df = pd.DataFrame(Y_pred_tuned_xgb_test, columns=Y_test.columns, index=Y_test.index)\n",
        "\n",
        "for i, target_col in enumerate(Y_test.columns):\n",
        "    r2 = r2_score(Y_test[target_col], Y_pred_tuned_xgb_test_df[target_col])\n",
        "    mae = mean_absolute_error(Y_test[target_col], Y_pred_tuned_xgb_test_df[target_col])\n",
        "    rmse = np.sqrt(mean_squared_error(Y_test[target_col], Y_pred_tuned_xgb_test_df[target_col]))\n",
        "\n",
        "    print(f\"\\nMetrics for Target: {target_col} (Test Set, Tuned XGBoost)\")\n",
        "    print(f\"  R-squared (R²): {r2:.4f}\")\n",
        "    print(f\"  Mean Absolute Error (MAE): {mae:.4f}\")\n",
        "    print(f\"  Root Mean Squared Error (RMSE): {rmse:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzIFq2zS0WT-",
        "outputId": "15b414dc-0adc-4421-99f7-53bb5803b9ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Final Evaluation of Tuned XGBoost Model on Test Set ---\n",
            "\n",
            "Metrics for Target: theoretical_capacity_grav (Test Set, Tuned XGBoost)\n",
            "  R-squared (R²): 0.6534\n",
            "  Mean Absolute Error (MAE): 18.2208\n",
            "  Root Mean Squared Error (RMSE): 42.0546\n",
            "\n",
            "Metrics for Target: average_voltage (Test Set, Tuned XGBoost)\n",
            "  R-squared (R²): 0.8449\n",
            "  Mean Absolute Error (MAE): 0.3676\n",
            "  Root Mean Squared Error (RMSE): 1.5609\n",
            "\n",
            "Metrics for Target: energy_above_hull (Test Set, Tuned XGBoost)\n",
            "  R-squared (R²): 0.5002\n",
            "  Mean Absolute Error (MAE): 0.0236\n",
            "  Root Mean Squared Error (RMSE): 0.0326\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing on dataset using tuned model"
      ],
      "metadata": {
        "id": "Zt-dpfA-IDmC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select a random index from the test set\n",
        "random_index = random.randint(0, len(X_test_scaled) - 1)\n",
        "\n",
        "# Get the actual features and targets for the random material\n",
        "random_material_features = X_test_scaled.iloc[[random_index]]\n",
        "random_material_actual_targets = Y_test.iloc[[random_index]]\n",
        "\n",
        "# Get the original material id and formula\n",
        "original_material_id = df_engineered.loc[random_material_features.index[0], 'material_id']\n",
        "original_formula_pretty = df_engineered.loc[random_material_features.index[0], 'formula_pretty']\n",
        "\n",
        "# Predict the targets for the random material using the best tuned model\n",
        "random_material_predicted_targets_scaled_tuned = best_multioutput_xgb.predict(random_material_features)\n",
        "\n",
        "\n",
        "print(f\"\\nTesting prediction on a random material using the TUNED model (Index: {random_index}):\")\n",
        "print(f\"  Material ID: {original_material_id}\")\n",
        "print(f\"  Formula: {original_formula_pretty}\")\n",
        "\n",
        "print(\"\\nActual Target Values:\")\n",
        "# Add units to the column names for actual targets\n",
        "actual_targets_with_units = random_material_actual_targets.copy()\n",
        "actual_targets_with_units.columns = ['theoretical_capacity_grav (mAh/g)', 'average_voltage (V)', 'energy_above_hull (eV/atom)']\n",
        "print(actual_targets_with_units.to_markdown(numalign=\"left\", stralign=\"left\"))\n",
        "\n",
        "\n",
        "print(\"\\nPredicted Target Values (Tuned Model):\")\n",
        "# Convert the numpy array prediction to a pandas DataFrame for easier printing\n",
        "predicted_df_tuned = pd.DataFrame(random_material_predicted_targets_scaled_tuned, columns=Y_cols, index=[0])\n",
        "# Add units to the column names for predicted targets\n",
        "predicted_df_tuned.columns = ['theoretical_capacity_grav (mAh/g)', 'average_voltage (V)', 'energy_above_hull (eV/atom)']\n",
        "print(predicted_df_tuned.to_markdown(numalign=\"left\", stralign=\"left\"))\n",
        "\n",
        "# Calculate absolute errors for this specific material using the tuned model\n",
        "abs_errors_tuned = np.abs(random_material_actual_targets.values - random_material_predicted_targets_scaled_tuned)\n",
        "# Add units to the column names for absolute errors\n",
        "abs_errors_tuned_df = pd.DataFrame(abs_errors_tuned, columns=[f'Absolute Error ({col})' for col in ['theoretical_capacity_grav (mAh/g)', 'average_voltage (V)', 'energy_above_hull (eV/atom)']], index=[0])\n",
        "\n",
        "print(\"\\nAbsolute Errors for this Prediction (Tuned Model):\")\n",
        "print(abs_errors_tuned_df.to_markdown(numalign=\"left\", stralign=\"left\"))\n",
        "\n",
        "# Calculate relative errors for this specific material using the tuned model\n",
        "# Avoid division by zero for actual values close to zero\n",
        "relative_errors_tuned = np.where(random_material_actual_targets.values != 0,\n",
        "                                 abs_errors_tuned / random_material_actual_targets.values,\n",
        "                                 np.nan) # Use NaN where actual value is 0\n",
        "\n",
        "# Add units to the column names for relative errors\n",
        "relative_errors_tuned_df = pd.DataFrame(relative_errors_tuned, columns=[f'Relative Error ({col})' for col in ['theoretical_capacity_grav (%)', 'average_voltage (%)', 'energy_above_hull (%)']], index=[0])\n",
        "\n",
        "print(\"\\nRelative Errors for this Prediction (Tuned Model, as %):\")\n",
        "# Convert relative error to percentage for display\n",
        "relative_errors_tuned_df = relative_errors_tuned_df * 100\n",
        "print(relative_errors_tuned_df.to_markdown(numalign=\"left\", stralign=\"left\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYsBanFEQesw",
        "outputId": "b9304290-80f9-49aa-e304-b5446356a34f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing prediction on a random material using the TUNED model (Index: 343):\n",
            "  Material ID: mp-32023\n",
            "  Formula: LiMn2(PO4)3\n",
            "\n",
            "Actual Target Values:\n",
            "|      | theoretical_capacity_grav (mAh/g)   | average_voltage (V)   | energy_above_hull (eV/atom)   |\n",
            "|:-----|:------------------------------------|:----------------------|:------------------------------|\n",
            "| 1175 | 128.973                             | 4.01943               | 0.0619951                     |\n",
            "\n",
            "Predicted Target Values (Tuned Model):\n",
            "|    | theoretical_capacity_grav (mAh/g)   | average_voltage (V)   | energy_above_hull (eV/atom)   |\n",
            "|:---|:------------------------------------|:----------------------|:------------------------------|\n",
            "| 0  | 160.25                              | 3.91649               | 0.0733415                     |\n",
            "\n",
            "Absolute Errors for this Prediction (Tuned Model):\n",
            "|    | Absolute Error (theoretical_capacity_grav (mAh/g))   | Absolute Error (average_voltage (V))   | Absolute Error (energy_above_hull (eV/atom))   |\n",
            "|:---|:-----------------------------------------------------|:---------------------------------------|:-----------------------------------------------|\n",
            "| 0  | 31.2767                                              | 0.102936                               | 0.0113464                                      |\n",
            "\n",
            "Relative Errors for this Prediction (Tuned Model, as %):\n",
            "|    | Relative Error (theoretical_capacity_grav (%))   | Relative Error (average_voltage (%))   | Relative Error (energy_above_hull (%))   |\n",
            "|:---|:-------------------------------------------------|:---------------------------------------|:-----------------------------------------|\n",
            "| 0  | 24.2506                                          | 2.56097                                | 18.3021                                  |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save model and scaler"
      ],
      "metadata": {
        "id": "iVOBPihJVtMN"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaec90f3",
        "outputId": "2a13724b-9db9-4d82-8d18-6f54d5ab7a9c"
      },
      "source": [
        "# Define the path for the models folder\n",
        "model_folder_path = '/content/drive/MyDrive/Colab Notebooks/Masters Research Project/Model'\n",
        "\n",
        "model_filename = os.path.join(model_folder_path, \"tuned_xgboost_multi_output_model_v2.joblib\")\n",
        "scaler_filename = os.path.join(model_folder_path, \"scaler_X_v2.joblib\")\n",
        "feature_names_filename = os.path.join(model_folder_path, \"feature_column_names_v2.joblib\")\n",
        "\n",
        "try:\n",
        "    joblib.dump(best_multioutput_xgb, model_filename)\n",
        "    joblib.dump(scaler_X, scaler_filename)\n",
        "    joblib.dump(X_train_scaled.columns.tolist(), feature_names_filename)\n",
        "\n",
        "    print(f\"Tuned XGBoost model saved to: '{model_filename}'\")\n",
        "    print(f\"Scaler saved to: '{scaler_filename}'\")\n",
        "    print(f\"Feature column names saved to: '{feature_names_filename}'\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving model or scaler: {e}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuned XGBoost model saved to: '/content/drive/MyDrive/Colab Notebooks/Masters Research Project/Model/tuned_xgboost_multi_output_model_v2.joblib'\n",
            "Scaler saved to: '/content/drive/MyDrive/Colab Notebooks/Masters Research Project/Model/scaler_X_v2.joblib'\n",
            "Feature column names saved to: '/content/drive/MyDrive/Colab Notebooks/Masters Research Project/Model/feature_column_names_v2.joblib'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path for saving the training data\n",
        "data_output_folder_path = '/content/drive/MyDrive/Colab Notebooks/Masters Research Project/Data/Training_Data'\n",
        "os.makedirs(data_output_folder_path, exist_ok=True) # Create the directory if it doesn't exist\n",
        "\n",
        "# Define filenames for the training data\n",
        "X_train_filename = os.path.join(data_output_folder_path, \"X_train_scaled.csv\")\n",
        "Y_train_filename = os.path.join(data_output_folder_path, \"Y_train.csv\")\n",
        "X_test_filename = os.path.join(data_output_folder_path, \"X_test_scaled.csv\")\n",
        "Y_test_filename = os.path.join(data_output_folder_path, \"Y_test.csv\")\n",
        "\n",
        "try:\n",
        "    # Add 'material_id' and 'formula_pretty' back to the scaled dataframes using their index\n",
        "    X_train_scaled_with_ids = X_train_scaled.copy()\n",
        "    X_test_scaled_with_ids = X_test_scaled.copy()\n",
        "\n",
        "    # Ensure the index is aligned with the original dataframe to get the correct IDs and formulas\n",
        "    X_train_scaled_with_ids[['material_id', 'formula_pretty']] = df_engineered.loc[X_train_scaled_with_ids.index, ['material_id', 'formula_pretty']]\n",
        "    X_test_scaled_with_ids[['material_id', 'formula_pretty']] = df_engineered.loc[X_test_scaled_with_ids.index, ['material_id', 'formula_pretty']]\n",
        "\n",
        "\n",
        "    # Save the training and test features (scaled) with IDs and formulas\n",
        "    X_train_scaled_with_ids.to_csv(X_train_filename, index=True) # Keep index to link back if needed\n",
        "    X_test_scaled_with_ids.to_csv(X_test_filename, index=True)   # Keep index to link back if needed\n",
        "\n",
        "    # Save the training and test targets\n",
        "    Y_train.to_csv(Y_train_filename, index=True) # Keep index to link back if needed\n",
        "    Y_test.to_csv(Y_test_filename, index=True)   # Keep index to link back if needed\n",
        "\n",
        "    print(f\"Training data (X_train_scaled_with_ids and Y_train) saved to: '{data_output_folder_path}'\")\n",
        "    print(f\"Test data (X_test_scaled_with_ids and Y_test) saved to: '{data_output_folder_path}'\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error saving training data: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9g_zm1Sy36W3",
        "outputId": "eb06570e-3117-4b72-ed54-59a63853f52b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data (X_train_scaled_with_ids and Y_train) saved to: '/content/drive/MyDrive/Colab Notebooks/Masters Research Project/Data/Training_Data'\n",
            "Test data (X_test_scaled_with_ids and Y_test) saved to: '/content/drive/MyDrive/Colab Notebooks/Masters Research Project/Data/Training_Data'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest"
      ],
      "metadata": {
        "id": "EtJsxgMkIbn-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the base Random Forest model\n",
        "rf_base_model = RandomForestRegressor(\n",
        "    n_estimators=300,  # Number of trees in the forest\n",
        "    max_depth=10,      # Maximum depth of the trees\n",
        "    min_samples_split=5, # Minimum number of samples required to split an internal node\n",
        "    min_samples_leaf=3,  # Minimum number of samples required to be at a leaf node\n",
        "    random_state=9,\n",
        "    n_jobs=-1          # Use all available CPU cores\n",
        ")\n",
        "\n",
        "# Wrap the Random Forest model in MultiOutputRegressor\n",
        "multioutput_rf = MultiOutputRegressor(rf_base_model)\n",
        "\n",
        "# Prepare lists to store scores for each target across all folds\n",
        "rf_target_r2_scores = {col: [] for col in Y_train.columns}\n",
        "rf_target_mae_scores = {col: [] for col in Y_train.columns}\n",
        "rf_target_rmse_scores = {col: [] for col in Y_train.columns}\n",
        "\n",
        "# Perform cross-validation with the Random Forest model\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=9)\n",
        "\n",
        "print(\"\\n--- Random Forest Cross-Validation ---\")\n",
        "for fold_idx, (train_index, test_index) in enumerate(kfold.split(X_train_scaled, Y_train)):\n",
        "    X_train_fold, X_test_fold = X_train_scaled.iloc[train_index], X_train_scaled.iloc[test_index]\n",
        "    Y_train_fold, Y_test_fold = Y_train.iloc[train_index], Y_train.iloc[test_index]\n",
        "\n",
        "    print(f\"Fold {fold_idx + 1}/5: Training...\")\n",
        "    # Fit a new instance of the MultiOutput Random Forest model for each fold\n",
        "    fold_multioutput_rf = MultiOutputRegressor(rf_base_model)\n",
        "    fold_multioutput_rf.fit(X_train_fold, Y_train_fold)\n",
        "\n",
        "    Y_pred_fold = fold_multioutput_rf.predict(X_test_fold)\n",
        "    # Convert predictions back to DataFrame with column names for clarity\n",
        "    Y_pred_fold_df = pd.DataFrame(Y_pred_fold, columns=Y_test_fold.columns, index=Y_test_fold.index)\n",
        "\n",
        "    print(f\"Fold {fold_idx + 1}/5: Evaluating...\")\n",
        "    for i, target_col in enumerate(Y_test_fold.columns):\n",
        "        r2 = r2_score(Y_test_fold[target_col], Y_pred_fold_df[target_col])\n",
        "        mae = mean_absolute_error(Y_test_fold[target_col], Y_pred_fold_df[target_col])\n",
        "        rmse = np.sqrt(mean_squared_error(Y_test_fold[target_col], Y_pred_fold_df[target_col]))\n",
        "\n",
        "        rf_target_r2_scores[target_col].append(r2)\n",
        "        rf_target_mae_scores[target_col].append(mae)\n",
        "        rf_target_rmse_scores[target_col].append(rmse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zphK8WA4IaJ_",
        "outputId": "374848d5-3a4e-426f-c602-26896d7a1807"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Random Forest Cross-Validation ---\n",
            "Fold 1/5: Training...\n",
            "Fold 1/5: Evaluating...\n",
            "Fold 2/5: Training...\n",
            "Fold 2/5: Evaluating...\n",
            "Fold 3/5: Training...\n",
            "Fold 3/5: Evaluating...\n",
            "Fold 4/5: Training...\n",
            "Fold 4/5: Evaluating...\n",
            "Fold 5/5: Training...\n",
            "Fold 5/5: Evaluating...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print aggregated cross-validation results for Random Forest\n",
        "print(\"\\n Random Forest Cross-Validation Results (Mean across 5 folds, Per Target)\")\n",
        "for target_col in Y_train.columns:\n",
        "    print(f\"\\nMetrics for Target: {target_col}\")\n",
        "    print(f\"  Mean R-squared (R²): {np.mean(rf_target_r2_scores[target_col]):.4f} (Std Dev: {np.std(rf_target_r2_scores[target_col]):.4f})\")\n",
        "    print(f\"  Mean MAE: {np.mean(rf_target_mae_scores[target_col]):.4f} (Std Dev: {np.std(rf_target_mae_scores[target_col]):.4f})\")\n",
        "    print(f\"  Mean RMSE: {np.mean(rf_target_rmse_scores[target_col]):.4f} (Std Dev: {np.std(rf_target_rmse_scores[target_col]):.4f})\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5kLZOo1Iz8s",
        "outputId": "57220fca-d5dc-4027-e1a6-5dd299c118ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Random Forest Cross-Validation Results (Mean across 5 folds, Per Target)\n",
            "\n",
            "Metrics for Target: theoretical_capacity_grav\n",
            "  Mean R-squared (R²): 0.8070 (Std Dev: 0.0544)\n",
            "  Mean MAE: 25.4524 (Std Dev: 1.6048)\n",
            "  Mean RMSE: 62.9646 (Std Dev: 15.4942)\n",
            "\n",
            "Metrics for Target: average_voltage\n",
            "  Mean R-squared (R²): 0.7313 (Std Dev: 0.2368)\n",
            "  Mean MAE: 0.5274 (Std Dev: 0.2312)\n",
            "  Mean RMSE: 3.4047 (Std Dev: 4.4331)\n",
            "\n",
            "Metrics for Target: energy_above_hull\n",
            "  Mean R-squared (R²): 0.3983 (Std Dev: 0.0159)\n",
            "  Mean MAE: 0.0268 (Std Dev: 0.0004)\n",
            "  Mean RMSE: 0.0351 (Std Dev: 0.0005)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing Random forest model on test set"
      ],
      "metadata": {
        "id": "SM_lFIytMrST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train on the full training set and evaluate on test set for a final view\n",
        "print(\"\\n Random Forest Model Evaluation on Test Set\")\n",
        "multioutput_rf_final = MultiOutputRegressor(rf_base_model)\n",
        "multioutput_rf_final.fit(X_train_scaled, Y_train) # Train on full training data\n",
        "\n",
        "Y_pred_rf_test = multioutput_rf_final.predict(X_test_scaled)\n",
        "Y_pred_rf_test_df = pd.DataFrame(Y_pred_rf_test, columns=Y_test.columns, index=Y_test.index)\n",
        "\n",
        "for i, target_col in enumerate(Y_test.columns):\n",
        "    r2 = r2_score(Y_test[target_col], Y_pred_rf_test_df[target_col])\n",
        "    mae = mean_absolute_error(Y_test[target_col], Y_pred_rf_test_df[target_col])\n",
        "    rmse = np.sqrt(mean_squared_error(Y_test[target_col], Y_pred_rf_test_df[target_col]))\n",
        "\n",
        "    print(f\"\\nMetrics for Target: {target_col} (Test Set)\")\n",
        "    print(f\"  R-squared (R²): {r2:.4f}\")\n",
        "    print(f\"  Mean Absolute Error (MAE): {mae:.4f}\")\n",
        "    print(f\"  Root Mean Squared Error (RMSE): {rmse:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIeWKxT0I1UA",
        "outputId": "9527b0f0-65fb-4b3f-a206-958065adb9f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Random Forest Model Evaluation on Test Set\n",
            "\n",
            "Metrics for Target: theoretical_capacity_grav (Test Set)\n",
            "  R-squared (R²): 0.5862\n",
            "  Mean Absolute Error (MAE): 22.4741\n",
            "  Root Mean Squared Error (RMSE): 45.9480\n",
            "\n",
            "Metrics for Target: average_voltage (Test Set)\n",
            "  R-squared (R²): 0.7463\n",
            "  Mean Absolute Error (MAE): 0.4692\n",
            "  Root Mean Squared Error (RMSE): 1.9962\n",
            "\n",
            "Metrics for Target: energy_above_hull (Test Set)\n",
            "  R-squared (R²): 0.4107\n",
            "  Mean Absolute Error (MAE): 0.0263\n",
            "  Root Mean Squared Error (RMSE): 0.0354\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter tuning on Random Forest model"
      ],
      "metadata": {
        "id": "J4zE2UgnMLGG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the base Random Forest model for tuning\n",
        "rf_base_model_tuned = RandomForestRegressor(\n",
        "    random_state=9,\n",
        "    n_jobs=-1 # Use all available CPU cores\n",
        ")\n",
        "\n",
        "# Wrap it in MultiOutputRegressor\n",
        "multioutput_rf_tuned = MultiOutputRegressor(rf_base_model_tuned)\n",
        "\n",
        "# Define the parameter distribution for RandomizedSearchCV for Random Forest\n",
        "param_distributions_rf = {\n",
        "    'estimator__n_estimators': [100, 200, 300], # Number of trees\n",
        "    'estimator__max_depth': [5, 10, 15], # Maximum depth or None for full growth\n",
        "    'estimator__min_samples_split': [2, 5], # Minimum samples to split a node\n",
        "    'estimator__min_samples_leaf': [1, 2], # Minimum samples at a leaf node\n",
        "    'estimator__bootstrap': [True, False] # Whether bootstrap samples are used\n",
        "}\n",
        "\n",
        "# Initialize RandomizedSearchCV for Random Forest\n",
        "random_search_rf = RandomizedSearchCV(\n",
        "    estimator=multioutput_rf_tuned,\n",
        "    param_distributions=param_distributions_rf,\n",
        "    n_iter=25, # Number of parameter settings that are sampled\n",
        "    cv=4,      # Number of folds in cross-validation\n",
        "    verbose=2,\n",
        "    random_state=9,\n",
        "    n_jobs=-1  # Use all available CPU cores\n",
        ")\n",
        "\n",
        "# Fit RandomizedSearchCV to the scaled training data and targets\n",
        "print(\"\\n--- Performing Randomized Search for Random Forest Hyperparameter Tuning ---\")\n",
        "random_search_rf.fit(X_train_scaled, Y_train)\n",
        "\n",
        "# Get the best parameters and the best score\n",
        "best_params_rf = random_search_rf.best_params_\n",
        "best_score_rf = random_search_rf.best_score_ # This is the mean score of the best estimator on the held out data.\n",
        "\n",
        "print(f\"\\nBest parameters found for Random Forest: {best_params_rf}\")\n",
        "print(f\"Best score found for Random Forest (mean CV score): {best_score_rf:.4f}\")\n",
        "\n",
        "# Re-evaluate the best model with cross-validation on all targets for robust metrics\n",
        "\n",
        "# Get the best estimator from the search\n",
        "best_multioutput_rf = random_search_rf.best_estimator_\n",
        "\n",
        "# Prepare lists to store scores for each target across all folds\n",
        "rf_tuned_target_r2_scores = {col: [] for col in Y_train.columns}\n",
        "rf_tuned_target_mae_scores = {col: [] for col in Y_train.columns}\n",
        "rf_tuned_target_rmse_scores = {col: [] for col in Y_train.columns}\n",
        "\n",
        "# Perform cross-validation with the best estimator\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=9)\n",
        "\n",
        "print(\"\\n--- Cross-Validation Results for Tuned Random Forest ---\")\n",
        "for fold_idx, (train_index, test_index) in enumerate(kfold.split(X_train_scaled, Y_train)):\n",
        "    X_train_fold, X_test_fold = X_train_scaled.iloc[train_index], X_train_scaled.iloc[test_index]\n",
        "    Y_train_fold, Y_test_fold = Y_train.iloc[train_index], Y_train.iloc[test_index]\n",
        "\n",
        "    print(f\"Fold {fold_idx + 1}/5: Training tuned model...\")\n",
        "    # Fit a new instance of the best model for each fold (important for proper CV)\n",
        "    fold_rf_base_model_tuned = RandomForestRegressor(\n",
        "        n_estimators=best_params_rf['estimator__n_estimators'],\n",
        "        max_depth=best_params_rf['estimator__max_depth'],\n",
        "        min_samples_split=best_params_rf['estimator__min_samples_split'],\n",
        "        min_samples_leaf=best_params_rf['estimator__min_samples_leaf'],\n",
        "        bootstrap=best_params_rf['estimator__bootstrap'],\n",
        "        random_state=9,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    fold_multioutput_rf_tuned = MultiOutputRegressor(fold_rf_base_model_tuned)\n",
        "    fold_multioutput_rf_tuned.fit(X_train_fold, Y_train_fold)\n",
        "\n",
        "    Y_pred_fold = fold_multioutput_rf_tuned.predict(X_test_fold)\n",
        "    Y_pred_fold_df = pd.DataFrame(Y_pred_fold, columns=Y_test_fold.columns, index=Y_test_fold.index)\n",
        "\n",
        "    print(f\"Fold {fold_idx + 1}/5: Evaluating tuned model...\")\n",
        "    for i, target_col in enumerate(Y_test_fold.columns):\n",
        "        r2 = r2_score(Y_test_fold[target_col], Y_pred_fold_df[target_col])\n",
        "        mae = mean_absolute_error(Y_test_fold[target_col], Y_pred_fold_df[target_col])\n",
        "        rmse = np.sqrt(mean_squared_error(Y_test_fold[target_col], Y_pred_fold_df[target_col]))\n",
        "\n",
        "        rf_tuned_target_r2_scores[target_col].append(r2)\n",
        "        rf_tuned_target_mae_scores[target_col].append(mae)\n",
        "        rf_tuned_target_rmse_scores[target_col].append(rmse)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aP7u9VmpMOqS",
        "outputId": "fd3216ff-ac33-4b08-80a4-5eaf69f12a87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Performing Randomized Search for Random Forest Hyperparameter Tuning ---\n",
            "Fitting 4 folds for each of 25 candidates, totalling 100 fits\n",
            "\n",
            "Best parameters found for Random Forest: {'estimator__n_estimators': 200, 'estimator__min_samples_split': 2, 'estimator__min_samples_leaf': 1, 'estimator__max_depth': 15, 'estimator__bootstrap': True}\n",
            "Best score found for Random Forest (mean CV score): 0.6524\n",
            "\n",
            "--- Cross-Validation Results for Tuned Random Forest ---\n",
            "Fold 1/5: Training tuned model...\n",
            "Fold 1/5: Evaluating tuned model...\n",
            "Fold 2/5: Training tuned model...\n",
            "Fold 2/5: Evaluating tuned model...\n",
            "Fold 3/5: Training tuned model...\n",
            "Fold 3/5: Evaluating tuned model...\n",
            "Fold 4/5: Training tuned model...\n",
            "Fold 4/5: Evaluating tuned model...\n",
            "Fold 5/5: Training tuned model...\n",
            "Fold 5/5: Evaluating tuned model...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print aggregated cross-validation results for the tuned Random Forest\n",
        "print(\"\\n--- Tuned Random Forest Cross-Validation Results (Mean across 5 folds, Per Target) ---\")\n",
        "for target_col in Y_train.columns:\n",
        "    print(f\"\\nMetrics for Target: {target_col}\")\n",
        "    print(f\"  Mean R-squared (R²): {np.mean(rf_tuned_target_r2_scores[target_col]):.4f} (Std Dev: {np.std(rf_tuned_target_r2_scores[target_col]):.4f})\")\n",
        "    print(f\"  Mean MAE: {np.mean(rf_tuned_target_mae_scores[target_col]):.4f} (Std Dev: {np.std(rf_tuned_target_mae_scores[target_col]):.4f})\")\n",
        "    print(f\"  Mean RMSE: {np.mean(rf_tuned_target_rmse_scores[target_col]):.4f} (Std Dev: {np.std(rf_tuned_target_rmse_scores[target_col]):.4f})\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0yNtIVGMbHO",
        "outputId": "dc0c5a4c-8145-43b7-8d65-79846fa48c56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Tuned Random Forest Cross-Validation Results (Mean across 5 folds, Per Target) ---\n",
            "\n",
            "Metrics for Target: theoretical_capacity_grav\n",
            "  Mean R-squared (R²): 0.8114 (Std Dev: 0.0472)\n",
            "  Mean MAE: 22.7258 (Std Dev: 1.6363)\n",
            "  Mean RMSE: 62.5175 (Std Dev: 15.0600)\n",
            "\n",
            "Metrics for Target: average_voltage\n",
            "  Mean R-squared (R²): 0.7934 (Std Dev: 0.1878)\n",
            "  Mean MAE: 0.4743 (Std Dev: 0.1984)\n",
            "  Mean RMSE: 3.0983 (Std Dev: 4.0410)\n",
            "\n",
            "Metrics for Target: energy_above_hull\n",
            "  Mean R-squared (R²): 0.4366 (Std Dev: 0.0128)\n",
            "  Mean MAE: 0.0256 (Std Dev: 0.0004)\n",
            "  Mean RMSE: 0.0340 (Std Dev: 0.0005)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing tuned random forest model on test set"
      ],
      "metadata": {
        "id": "h4ySH3lcMla7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Final Evaluation on the untouched Test Set with the best tuned model\n",
        "print(\"\\n--- Final Evaluation of Tuned Random Forest Model on Test Set ---\")\n",
        "Y_pred_tuned_rf_test = best_multioutput_rf.predict(X_test_scaled)\n",
        "Y_pred_tuned_rf_test_df = pd.DataFrame(Y_pred_tuned_rf_test, columns=Y_test.columns, index=Y_test.index)\n",
        "\n",
        "for i, target_col in enumerate(Y_test.columns):\n",
        "    r2 = r2_score(Y_test[target_col], Y_pred_tuned_rf_test_df[target_col])\n",
        "    mae = mean_absolute_error(Y_test[target_col], Y_pred_tuned_rf_test_df[target_col])\n",
        "    rmse = np.sqrt(mean_squared_error(Y_test[target_col], Y_pred_tuned_rf_test_df[target_col]))\n",
        "\n",
        "    print(f\"\\nMetrics for Target: {target_col} (Test Set, Tuned Random Forest)\")\n",
        "    print(f\"  R-squared (R²): {r2:.4f}\")\n",
        "    print(f\"  Mean Absolute Error (MAE): {mae:.4f}\")\n",
        "    print(f\"  Root Mean Squared Error (RMSE): {rmse:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7OMf64guMcQ8",
        "outputId": "9e080576-f01c-4b78-9cda-f74ee0dd44c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Final Evaluation of Tuned Random Forest Model on Test Set ---\n",
            "\n",
            "Metrics for Target: theoretical_capacity_grav (Test Set, Tuned Random Forest)\n",
            "  R-squared (R²): 0.6406\n",
            "  Mean Absolute Error (MAE): 19.0836\n",
            "  Root Mean Squared Error (RMSE): 42.8224\n",
            "\n",
            "Metrics for Target: average_voltage (Test Set, Tuned Random Forest)\n",
            "  R-squared (R²): 0.8924\n",
            "  Mean Absolute Error (MAE): 0.3814\n",
            "  Root Mean Squared Error (RMSE): 1.3002\n",
            "\n",
            "Metrics for Target: energy_above_hull (Test Set, Tuned Random Forest)\n",
            "  R-squared (R²): 0.4496\n",
            "  Mean Absolute Error (MAE): 0.0251\n",
            "  Root Mean Squared Error (RMSE): 0.0342\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Tuned Random Forest model"
      ],
      "metadata": {
        "id": "RTawfhZSSt7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path for the models folder\n",
        "model_folder_path = '/content/drive/MyDrive/Colab Notebooks/Masters Research Project/Model'\n",
        "os.makedirs(model_folder_path, exist_ok=True) # Create the directory if it doesn't exist\n",
        "\n",
        "# Define the filename for the tuned Random Forest model\n",
        "rf_model_filename_tuned = os.path.join(model_folder_path, \"tuned_random_forest_multi_output_model_v2.joblib\")\n",
        "\n",
        "try:\n",
        "    # Save the best tuned Random Forest model\n",
        "    joblib.dump(best_multioutput_rf, rf_model_filename_tuned)\n",
        "\n",
        "    print(f\"Tuned Random Forest model saved to: '{rf_model_filename_tuned}'\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error saving tuned Random Forest model: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jr15cBemSst4",
        "outputId": "ba8c5872-0499-4185-c9d3-235cdc585b22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuned Random Forest model saved to: '/content/drive/MyDrive/Colab Notebooks/Masters Research Project/Model/tuned_random_forest_multi_output_model_v2.joblib'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LightGBM"
      ],
      "metadata": {
        "id": "ltXOM9jyI6_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the base LightGBM model\n",
        "lgb_base_model = lgb.LGBMRegressor(\n",
        "    objective='regression_l1', # MAE objective\n",
        "    metric='mae',              # Metric for evaluation during training\n",
        "    n_estimators=300,          # A good starting point for boosting rounds\n",
        "    learning_rate=0.1,         # Step size shrinkage\n",
        "    num_leaves=31,             # Maximum number of leaves in one tree\n",
        "    max_depth=-1,              # No limit on tree depth\n",
        "    min_child_samples=20,      # Minimum number of data needed in a child\n",
        "    subsample=0.8,             # Fraction of samples for fitting the individual base learners\n",
        "    colsample_bytree=0.8,      # Fraction of features for fitting the individual base learners\n",
        "    random_state=9,\n",
        "    n_jobs=-1,\n",
        "    boosting_type='gbdt'       # Traditional Gradient Boosting Decision Tree\n",
        ")\n",
        "\n",
        "# Wrap the LightGBM model in MultiOutputRegressor\n",
        "multioutput_lgb = MultiOutputRegressor(lgb_base_model)\n",
        "\n",
        "# Prepare lists to store scores for each target across all folds\n",
        "lgb_target_r2_scores = {col: [] for col in Y_train.columns}\n",
        "lgb_target_mae_scores = {col: [] for col in Y_train.columns}\n",
        "lgb_target_rmse_scores = {col: [] for col in Y_train.columns}\n",
        "\n",
        "# Perform cross-validation with the LightGBM model\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=9)\n",
        "\n",
        "print(\"\\n--- LightGBM Cross-Validation ---\")\n",
        "for fold_idx, (train_index, test_index) in enumerate(kfold.split(X_train_scaled, Y_train)):\n",
        "    X_train_fold, X_test_fold = X_train_scaled.iloc[train_index], X_train_scaled.iloc[test_index]\n",
        "    Y_train_fold, Y_test_fold = Y_train.iloc[train_index], Y_train.iloc[test_index]\n",
        "\n",
        "    print(f\"Fold {fold_idx + 1}/5: Training...\")\n",
        "    # Fit a new instance of the MultiOutput LightGBM model for each fold\n",
        "    fold_multioutput_lgb = MultiOutputRegressor(lgb_base_model)\n",
        "    fold_multioutput_lgb.fit(X_train_fold, Y_train_fold)\n",
        "\n",
        "    Y_pred_fold = fold_multioutput_lgb.predict(X_test_fold)\n",
        "    # Convert predictions back to DataFrame with column names for clarity\n",
        "    Y_pred_fold_df = pd.DataFrame(Y_pred_fold, columns=Y_test_fold.columns, index=Y_test_fold.index)\n",
        "\n",
        "    print(f\"Fold {fold_idx + 1}/5: Evaluating...\")\n",
        "    for i, target_col in enumerate(Y_test_fold.columns):\n",
        "        r2 = r2_score(Y_test_fold[target_col], Y_pred_fold_df[target_col])\n",
        "        mae = mean_absolute_error(Y_test_fold[target_col], Y_pred_fold_df[target_col])\n",
        "        rmse = np.sqrt(mean_squared_error(Y_test_fold[target_col], Y_pred_fold_df[target_col]))\n",
        "\n",
        "        lgb_target_r2_scores[target_col].append(r2)\n",
        "        lgb_target_mae_scores[target_col].append(mae)\n",
        "        lgb_target_rmse_scores[target_col].append(rmse)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLxHet5wI88z",
        "outputId": "4465795a-2161-437b-d79a-eefb056b9607"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- LightGBM Cross-Validation ---\n",
            "Fold 1/5: Training...\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001089 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4415\n",
            "[LightGBM] [Info] Number of data points in the train set: 2204, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 111.869675\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001079 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4415\n",
            "[LightGBM] [Info] Number of data points in the train set: 2204, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 2.491324\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001049 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4415\n",
            "[LightGBM] [Info] Number of data points in the train set: 2204, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 0.067682\n",
            "Fold 1/5: Evaluating...\n",
            "Fold 2/5: Training...\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001021 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4421\n",
            "[LightGBM] [Info] Number of data points in the train set: 2205, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 111.743111\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001046 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4421\n",
            "[LightGBM] [Info] Number of data points in the train set: 2205, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 2.479698\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001067 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4421\n",
            "[LightGBM] [Info] Number of data points in the train set: 2205, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 0.067565\n",
            "Fold 2/5: Evaluating...\n",
            "Fold 3/5: Training...\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004723 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4417\n",
            "[LightGBM] [Info] Number of data points in the train set: 2205, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 111.102219\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001047 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4417\n",
            "[LightGBM] [Info] Number of data points in the train set: 2205, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 2.484276\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001135 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4417\n",
            "[LightGBM] [Info] Number of data points in the train set: 2205, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 0.066839\n",
            "Fold 3/5: Evaluating...\n",
            "Fold 4/5: Training...\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001017 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4428\n",
            "[LightGBM] [Info] Number of data points in the train set: 2205, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 111.644089\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001030 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4428\n",
            "[LightGBM] [Info] Number of data points in the train set: 2205, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 2.535259\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001040 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4428\n",
            "[LightGBM] [Info] Number of data points in the train set: 2205, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 0.066440\n",
            "Fold 4/5: Evaluating...\n",
            "Fold 5/5: Training...\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001015 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4419\n",
            "[LightGBM] [Info] Number of data points in the train set: 2205, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 110.517105\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001033 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4419\n",
            "[LightGBM] [Info] Number of data points in the train set: 2205, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 2.511411\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001068 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4419\n",
            "[LightGBM] [Info] Number of data points in the train set: 2205, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 0.066928\n",
            "Fold 5/5: Evaluating...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print aggregated cross-validation results for LightGBM\n",
        "print(\"\\n LightGBM Cross-Validation Results (Mean across 5 folds, Per Target)\")\n",
        "for target_col in Y_train.columns:\n",
        "    print(f\"\\nMetrics for Target: {target_col}\")\n",
        "    print(f\"  Mean R-squared (R²): {np.mean(lgb_target_r2_scores[target_col]):.4f} (Std Dev: {np.std(lgb_target_r2_scores[target_col]):.4f})\")\n",
        "    print(f\"  Mean MAE: {np.mean(lgb_target_mae_scores[target_col]):.4f} (Std Dev: {np.std(lgb_target_mae_scores[target_col]):.4f})\")\n",
        "    print(f\"  Mean RMSE: {np.mean(lgb_target_rmse_scores[target_col]):.4f} (Std Dev: {np.std(lgb_target_rmse_scores[target_col]):.4f})\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwuBLyWNTKx1",
        "outputId": "b8b73bca-a902-4986-955b-4fcd450840dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " LightGBM Cross-Validation Results (Mean across 5 folds, Per Target)\n",
            "\n",
            "Metrics for Target: theoretical_capacity_grav\n",
            "  Mean R-squared (R²): 0.6299 (Std Dev: 0.0792)\n",
            "  Mean MAE: 26.0190 (Std Dev: 3.5175)\n",
            "  Mean RMSE: 90.1887 (Std Dev: 29.9654)\n",
            "\n",
            "Metrics for Target: average_voltage\n",
            "  Mean R-squared (R²): 0.3901 (Std Dev: 0.3152)\n",
            "  Mean MAE: 0.5661 (Std Dev: 0.2717)\n",
            "  Mean RMSE: 4.8432 (Std Dev: 4.9925)\n",
            "\n",
            "Metrics for Target: energy_above_hull\n",
            "  Mean R-squared (R²): 0.4469 (Std Dev: 0.0193)\n",
            "  Mean MAE: 0.0247 (Std Dev: 0.0005)\n",
            "  Mean RMSE: 0.0337 (Std Dev: 0.0008)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing LightGBM model on test set"
      ],
      "metadata": {
        "id": "LnLIg05nTvFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train on the full training set and evaluate on test set for a final view\n",
        "print(\"\\n LightGBM Model Evaluation on Test Set\")\n",
        "multioutput_lgb_final = MultiOutputRegressor(lgb_base_model)\n",
        "multioutput_lgb_final.fit(X_train_scaled, Y_train) # Train on full training data\n",
        "\n",
        "Y_pred_lgb_test = multioutput_lgb_final.predict(X_test_scaled)\n",
        "Y_pred_lgb_test_df = pd.DataFrame(Y_pred_lgb_test, columns=Y_test.columns, index=Y_test.index)\n",
        "\n",
        "for i, target_col in enumerate(Y_test.columns):\n",
        "    r2 = r2_score(Y_test[target_col], Y_pred_lgb_test_df[target_col])\n",
        "    mae = mean_absolute_error(Y_test[target_col], Y_pred_lgb_test_df[target_col])\n",
        "    rmse = np.sqrt(mean_squared_error(Y_test[target_col], Y_pred_lgb_test_df[target_col]))\n",
        "\n",
        "    print(f\"\\nMetrics for Target: {target_col} (Test Set)\")\n",
        "    print(f\"  R-squared (R²): {r2:.4f}\")\n",
        "    print(f\"  Mean Absolute Error (MAE): {mae:.4f}\")\n",
        "    print(f\"  Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6Y_1hSVTONs",
        "outputId": "2a579cda-53a3-4cb9-fc94-44ef17b90f30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " LightGBM Model Evaluation on Test Set\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001228 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4497\n",
            "[LightGBM] [Info] Number of data points in the train set: 2756, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 111.441483\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001214 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4497\n",
            "[LightGBM] [Info] Number of data points in the train set: 2756, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 2.504560\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001206 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4497\n",
            "[LightGBM] [Info] Number of data points in the train set: 2756, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 0.067028\n",
            "\n",
            "Metrics for Target: theoretical_capacity_grav (Test Set)\n",
            "  R-squared (R²): 0.6681\n",
            "  Mean Absolute Error (MAE): 19.8319\n",
            "  Root Mean Squared Error (RMSE): 41.1538\n",
            "\n",
            "Metrics for Target: average_voltage (Test Set)\n",
            "  R-squared (R²): 0.3756\n",
            "  Mean Absolute Error (MAE): 0.5024\n",
            "  Root Mean Squared Error (RMSE): 3.1315\n",
            "\n",
            "Metrics for Target: energy_above_hull (Test Set)\n",
            "  R-squared (R²): 0.4817\n",
            "  Mean Absolute Error (MAE): 0.0239\n",
            "  Root Mean Squared Error (RMSE): 0.0332\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter tuning for LightGBM model"
      ],
      "metadata": {
        "id": "SmH_21hmTUGt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the base LightGBM model for tuning\n",
        "lgb_base_model_tuned = lgb.LGBMRegressor(\n",
        "    objective='regression_l1', # MAE objective\n",
        "    metric='mae',              # Metric for evaluation during training\n",
        "    random_state=9,\n",
        "    n_jobs=-1,\n",
        "    boosting_type='gbdt'\n",
        ")\n",
        "\n",
        "# Wrap it in MultiOutputRegressor\n",
        "multioutput_lgb_tuned = MultiOutputRegressor(lgb_base_model_tuned)\n",
        "\n",
        "# Define the parameter distribution for RandomizedSearchCV for LightGBM\n",
        "param_distributions_lgb = {\n",
        "    'estimator__n_estimators': [100, 200, 300], # Number of boosting rounds\n",
        "    'estimator__learning_rate': [0.01, 0.05, 0.1], # Step size shrinkage\n",
        "    'estimator__num_leaves': [10, 20, 40], # Maximum number of leaves\n",
        "    'estimator__max_depth': [-1, 5, 8, 10, 12], # Maximum depth or -1 for no limit\n",
        "    'estimator__min_child_samples': [10, 20, 30], # Minimum data needed in a child\n",
        "    'estimator__subsample': [0.6, 0.7, 0.8, 0.9, 1.0], # Subsample ratio of the training instance\n",
        "    'estimator__colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0], # Subsample ratio of columns\n",
        "    'estimator__reg_alpha': [0, 0.005, 0.01, 0.05, 0.1, 0.5, 1], # L1 regularization\n",
        "    'estimator__reg_lambda': [0, 0.005, 0.01, 0.05, 0.1, 0.5, 1] # L2 regularization\n",
        "}\n",
        "\n",
        "# Initialize RandomizedSearchCV for LightGBM\n",
        "random_search_lgb = RandomizedSearchCV(\n",
        "    estimator=multioutput_lgb_tuned,\n",
        "    param_distributions=param_distributions_lgb,\n",
        "    n_iter=25, # Number of parameter settings that are sampled\n",
        "    cv=4,      # Number of folds in cross-validation\n",
        "    verbose=2,\n",
        "    random_state=9,\n",
        "    n_jobs=-1  # Use all available CPU cores\n",
        ")\n",
        "\n",
        "# Fit RandomizedSearchCV to the scaled training data and targets\n",
        "print(\"\\n--- Performing Randomized Search for LightGBM Hyperparameter Tuning ---\")\n",
        "random_search_lgb.fit(X_train_scaled, Y_train)\n",
        "\n",
        "# Get the best parameters and the best score\n",
        "best_params_lgb = random_search_lgb.best_params_\n",
        "best_score_lgb = random_search_lgb.best_score_ # This is the mean score of the best estimator on the held out data.\n",
        "\n",
        "print(f\"\\nBest parameters found for LightGBM: {best_params_lgb}\")\n",
        "print(f\"Best score found for LightGBM (mean CV score): {best_score_lgb:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFI_zEAKTSfP",
        "outputId": "bee61cb4-aff1-4b12-8696-29a8cdef0b9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Performing Randomized Search for LightGBM Hyperparameter Tuning ---\n",
            "Fitting 4 folds for each of 25 candidates, totalling 100 fits\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001222 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4497\n",
            "[LightGBM] [Info] Number of data points in the train set: 2756, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 111.441483\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001267 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4497\n",
            "[LightGBM] [Info] Number of data points in the train set: 2756, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 2.504560\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001239 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4497\n",
            "[LightGBM] [Info] Number of data points in the train set: 2756, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 0.067028\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "\n",
            "Best parameters found for LightGBM: {'estimator__subsample': 0.7, 'estimator__reg_lambda': 0, 'estimator__reg_alpha': 0.01, 'estimator__num_leaves': 20, 'estimator__n_estimators': 300, 'estimator__min_child_samples': 10, 'estimator__max_depth': 10, 'estimator__learning_rate': 0.1, 'estimator__colsample_bytree': 0.7}\n",
            "Best score found for LightGBM (mean CV score): 0.5231\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-evaluate the best model with cross-validation on all targets for robust metrics\n",
        "\n",
        "# Get the best estimator from the search\n",
        "best_multioutput_lgb = random_search_lgb.best_estimator_\n",
        "\n",
        "# Prepare lists to store scores for each target across all folds\n",
        "lgb_tuned_target_r2_scores = {col: [] for col in Y_train.columns}\n",
        "lgb_tuned_target_mae_scores = {col: [] for col in Y_train.columns}\n",
        "lgb_tuned_target_rmse_scores = {col: [] for col in Y_train.columns}\n",
        "\n",
        "# Perform cross-validation with the best estimator\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=9)\n",
        "\n",
        "print(\"\\n--- Cross-Validation Results for Tuned LightGBM ---\")\n",
        "for fold_idx, (train_index, test_index) in enumerate(kfold.split(X_train_scaled, Y_train)):\n",
        "    X_train_fold, X_test_fold = X_train_scaled.iloc[train_index], X_train_scaled.iloc[test_index]\n",
        "    Y_train_fold, Y_test_fold = Y_train.iloc[train_index], Y_train.iloc[test_index]\n",
        "\n",
        "    print(f\"Fold {fold_idx + 1}/5: Training tuned model...\")\n",
        "    # Fit a new instance of the best model for each fold (important for proper CV)\n",
        "    fold_lgb_base_model_tuned = lgb.LGBMRegressor(\n",
        "        objective='regression_l1', # MAE objective\n",
        "        metric='mae',              # Metric for evaluation during training\n",
        "        n_estimators=best_params_lgb['estimator__n_estimators'],\n",
        "        learning_rate=best_params_lgb['estimator__learning_rate'],\n",
        "        num_leaves=best_params_lgb['estimator__num_leaves'],\n",
        "        max_depth=best_params_lgb['estimator__max_depth'],\n",
        "        min_child_samples=best_params_lgb['estimator__min_child_samples'],\n",
        "        subsample=best_params_lgb['estimator__subsample'],\n",
        "        colsample_bytree=best_params_lgb['estimator__colsample_bytree'],\n",
        "        reg_alpha=best_params_lgb['estimator__reg_alpha'],\n",
        "        reg_lambda=best_params_lgb['estimator__reg_lambda'],\n",
        "        random_state=9,\n",
        "        n_jobs=-1,\n",
        "        boosting_type='gbdt'\n",
        "    )\n",
        "    fold_multioutput_lgb_tuned = MultiOutputRegressor(fold_lgb_base_model_tuned)\n",
        "    fold_multioutput_lgb_tuned.fit(X_train_fold, Y_train_fold)\n",
        "\n",
        "    Y_pred_fold = fold_multioutput_lgb_tuned.predict(X_test_fold)\n",
        "    Y_pred_fold_df = pd.DataFrame(Y_pred_fold, columns=Y_test_fold.columns, index=Y_test_fold.index)\n",
        "\n",
        "    print(f\"Fold {fold_idx + 1}/5: Evaluating tuned model...\")\n",
        "    for i, target_col in enumerate(Y_test_fold.columns):\n",
        "        r2 = r2_score(Y_test_fold[target_col], Y_pred_fold_df[target_col])\n",
        "        mae = mean_absolute_error(Y_test_fold[target_col], Y_pred_fold_df[target_col])\n",
        "        rmse = np.sqrt(mean_squared_error(Y_test_fold[target_col], Y_pred_fold_df[target_col]))\n",
        "\n",
        "        lgb_tuned_target_r2_scores[target_col].append(r2)\n",
        "        lgb_tuned_target_mae_scores[target_col].append(mae)\n",
        "        lgb_tuned_target_rmse_scores[target_col].append(rmse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ap1sd49LT3tW",
        "outputId": "4df578bd-8527-4fea-e753-30f23c0a3e3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Cross-Validation Results for Tuned LightGBM ---\n",
            "Fold 1/5: Training tuned model...\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001075 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4415\n",
            "[LightGBM] [Info] Number of data points in the train set: 2204, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 111.869675\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001068 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4415\n",
            "[LightGBM] [Info] Number of data points in the train set: 2204, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 2.491324\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000977 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4415\n",
            "[LightGBM] [Info] Number of data points in the train set: 2204, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 0.067682\n",
            "Fold 1/5: Evaluating tuned model...\n",
            "Fold 2/5: Training tuned model...\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001015 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4421\n",
            "[LightGBM] [Info] Number of data points in the train set: 2205, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 111.743111\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000989 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4421\n",
            "[LightGBM] [Info] Number of data points in the train set: 2205, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 2.479698\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001041 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4421\n",
            "[LightGBM] [Info] Number of data points in the train set: 2205, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 0.067565\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Fold 2/5: Evaluating tuned model...\n",
            "Fold 3/5: Training tuned model...\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000968 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4417\n",
            "[LightGBM] [Info] Number of data points in the train set: 2205, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 111.102219\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001006 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4417\n",
            "[LightGBM] [Info] Number of data points in the train set: 2205, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 2.484276\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001011 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4417\n",
            "[LightGBM] [Info] Number of data points in the train set: 2205, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 0.066839\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Fold 3/5: Evaluating tuned model...\n",
            "Fold 4/5: Training tuned model...\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001048 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4428\n",
            "[LightGBM] [Info] Number of data points in the train set: 2205, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 111.644089\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001041 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4428\n",
            "[LightGBM] [Info] Number of data points in the train set: 2205, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 2.535259\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001006 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4428\n",
            "[LightGBM] [Info] Number of data points in the train set: 2205, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 0.066440\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Fold 4/5: Evaluating tuned model...\n",
            "Fold 5/5: Training tuned model...\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001041 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4419\n",
            "[LightGBM] [Info] Number of data points in the train set: 2205, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 110.517105\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001017 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4419\n",
            "[LightGBM] [Info] Number of data points in the train set: 2205, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 2.511411\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001098 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4419\n",
            "[LightGBM] [Info] Number of data points in the train set: 2205, number of used features: 49\n",
            "[LightGBM] [Info] Start training from score 0.066928\n",
            "Fold 5/5: Evaluating tuned model...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print aggregated cross-validation results for the tuned LightGBM\n",
        "print(\"\\n--- Tuned LightGBM Cross-Validation Results (Mean across 5 folds, Per Target) ---\")\n",
        "for target_col in Y_train.columns:\n",
        "    print(f\"\\nMetrics for Target: {target_col}\")\n",
        "    print(f\"  Mean R-squared (R²): {np.mean(lgb_tuned_target_r2_scores[target_col]):.4f} (Std Dev: {np.std(lgb_tuned_target_r2_scores[target_col]):.4f})\")\n",
        "    print(f\"  Mean MAE: {np.mean(lgb_tuned_target_mae_scores[target_col]):.4f} (Std Dev: {np.std(lgb_tuned_target_mae_scores[target_col]):.4f})\")\n",
        "    print(f\"  Mean RMSE: {np.mean(lgb_tuned_target_rmse_scores[target_col]):.4f} (Std Dev: {np.std(lgb_tuned_target_rmse_scores[target_col]):.4f})\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOHafMxZTluo",
        "outputId": "62e338de-69d6-4e97-9050-6483d203733a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Tuned LightGBM Cross-Validation Results (Mean across 5 folds, Per Target) ---\n",
            "\n",
            "Metrics for Target: theoretical_capacity_grav\n",
            "  Mean R-squared (R²): 0.7881 (Std Dev: 0.0630)\n",
            "  Mean MAE: 23.6727 (Std Dev: 2.4109)\n",
            "  Mean RMSE: 65.4221 (Std Dev: 12.6052)\n",
            "\n",
            "Metrics for Target: average_voltage\n",
            "  Mean R-squared (R²): 0.5229 (Std Dev: 0.2852)\n",
            "  Mean MAE: 0.5522 (Std Dev: 0.2745)\n",
            "  Mean RMSE: 4.4684 (Std Dev: 4.9971)\n",
            "\n",
            "Metrics for Target: energy_above_hull\n",
            "  Mean R-squared (R²): 0.4270 (Std Dev: 0.0202)\n",
            "  Mean MAE: 0.0252 (Std Dev: 0.0005)\n",
            "  Mean RMSE: 0.0343 (Std Dev: 0.0009)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing tuned LightGBM model on test set"
      ],
      "metadata": {
        "id": "_fQnBz7iT7Jz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Final Evaluation on the untouched Test Set with the best tuned model\n",
        "print(\"\\n--- Final Evaluation of Tuned LightGBM Model on Test Set ---\")\n",
        "Y_pred_tuned_lgb_test = best_multioutput_lgb.predict(X_test_scaled)\n",
        "Y_pred_tuned_lgb_test_df = pd.DataFrame(Y_pred_tuned_lgb_test, columns=Y_test.columns, index=Y_test.index)\n",
        "\n",
        "for i, target_col in enumerate(Y_test.columns):\n",
        "    r2 = r2_score(Y_test[target_col], Y_pred_tuned_lgb_test_df[target_col])\n",
        "    mae = mean_absolute_error(Y_test[target_col], Y_pred_tuned_lgb_test_df[target_col])\n",
        "    rmse = np.sqrt(mean_squared_error(Y_test[target_col], Y_pred_tuned_lgb_test_df[target_col]))\n",
        "\n",
        "    print(f\"\\nMetrics for Target: {target_col} (Test Set, Tuned LightGBM)\")\n",
        "    print(f\"  R-squared (R²): {r2:.4f}\")\n",
        "    print(f\"  Mean Absolute Error (MAE): {mae:.4f}\")\n",
        "    print(f\"  Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0tx-SSKTnM-",
        "outputId": "c50fba93-bd25-461a-cea4-7f0f49d69d45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Final Evaluation of Tuned LightGBM Model on Test Set ---\n",
            "\n",
            "Metrics for Target: theoretical_capacity_grav (Test Set, Tuned LightGBM)\n",
            "  R-squared (R²): 0.8093\n",
            "  Mean Absolute Error (MAE): 18.0919\n",
            "  Root Mean Squared Error (RMSE): 31.1952\n",
            "\n",
            "Metrics for Target: average_voltage (Test Set, Tuned LightGBM)\n",
            "  R-squared (R²): 0.5223\n",
            "  Mean Absolute Error (MAE): 0.4794\n",
            "  Root Mean Squared Error (RMSE): 2.7391\n",
            "\n",
            "Metrics for Target: energy_above_hull (Test Set, Tuned LightGBM)\n",
            "  R-squared (R²): 0.4505\n",
            "  Mean Absolute Error (MAE): 0.0247\n",
            "  Root Mean Squared Error (RMSE): 0.0342\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Tuned LightGBM model"
      ],
      "metadata": {
        "id": "ue7FpSOAT-Yb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path for the models folder\n",
        "model_folder_path = '/content/drive/MyDrive/Colab Notebooks/Masters Research Project/Model'\n",
        "os.makedirs(model_folder_path, exist_ok=True) # Create the directory if it doesn't exist\n",
        "\n",
        "# Define the filename for the tuned LightGBM model\n",
        "lgb_model_filename_tuned = os.path.join(model_folder_path, \"tuned_lightgbm_multi_output_model_v2.joblib\")\n",
        "\n",
        "try:\n",
        "    # Save the best tuned LightGBM model\n",
        "    joblib.dump(best_multioutput_lgb, lgb_model_filename_tuned)\n",
        "\n",
        "    print(f\"Tuned LightGBM model saved to: '{lgb_model_filename_tuned}'\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error saving tuned LightGBM model: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bh0tR8QcToWy",
        "outputId": "374ae326-b6da-44d2-e1cb-4de820faab1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuned LightGBM model saved to: '/content/drive/MyDrive/Colab Notebooks/Masters Research Project/Model/tuned_lightgbm_multi_output_model_v2.joblib'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10392ee2"
      },
      "source": [
        "# Appendix\n",
        "\n",
        "This appendix provides additional details and resources related to the analysis performed in this notebook.\n",
        "\n",
        "## A. Models\n",
        "\n",
        "Details about the models trained and evaluated in this notebook.\n",
        "\n",
        "### A.1 XGBoost Model\n",
        "\n",
        "- **Base Model Configuration:**\n",
        "  - Objective: `reg:squarederror`\n",
        "  - n_estimators: 300\n",
        "  - learning_rate: 0.1\n",
        "  - max_depth: 6\n",
        "  - subsample: 0.8\n",
        "  - colsample_bytree: 0.8\n",
        "  - random_state: 9\n",
        "  - n_jobs: -1\n",
        "\n",
        "- **Tuned Model Hyperparameters:**\n",
        "  - estimator__subsample: 0.8\n",
        "  - estimator__reg_lambda: 1\n",
        "  - estimator__reg_alpha: 0\n",
        "  - estimator__n_estimators: 200\n",
        "  - estimator__max_depth: 8\n",
        "  - estimator__learning_rate: 0.15\n",
        "  - estimator__gamma: 0\n",
        "  - estimator__colsample_bytree: 1.0\n",
        "\n",
        "- **Cross-Validation Results (Tuned Model):**\n",
        "  - theoretical_capacity_grav:\n",
        "    - Mean R²: 0.8705 (Std Dev: 0.0248)\n",
        "    - Mean MAE: 19.9334 (Std Dev: 1.4438)\n",
        "    - Mean RMSE: 52.2823 (Std Dev: 12.1327)\n",
        "  - average_voltage:\n",
        "    - Mean R²: 0.8067 (Std Dev: 0.1540)\n",
        "    - Mean MAE: 0.4362 (Std Dev: 0.1741)\n",
        "    - Mean RMSE: 2.8908 (Std Dev: 3.6284)\n",
        "  - energy_above_hull:\n",
        "    - Mean R²: 0.4592 (Std Dev: 0.0350)\n",
        "    - Mean MAE: 0.0246 (Std Dev: 0.0009)\n",
        "    - Mean RMSE: 0.0333 (Std Dev: 0.0011)\n",
        "\n",
        "- **Test Set Evaluation (Tuned Model):**\n",
        "  - theoretical_capacity_grav:\n",
        "    - R²: 0.6534\n",
        "    - MAE: 18.2208\n",
        "    - RMSE: 42.0546\n",
        "  - average_voltage:\n",
        "    - R²: 0.8449\n",
        "    - MAE: 0.3676\n",
        "    - RMSE: 1.5609\n",
        "  - energy_above_hull:\n",
        "    - R²: 0.5002\n",
        "    - MAE: 0.0236\n",
        "    - RMSE: 0.0326\n",
        "\n",
        "### A.2 Random Forest Model\n",
        "\n",
        "- **Base Model Configuration:**\n",
        "  - n_estimators: 300\n",
        "  - max_depth: 10\n",
        "  - min_samples_split: 5\n",
        "  - min_samples_leaf: 3\n",
        "  - random_state: 9\n",
        "  - n_jobs: -1\n",
        "\n",
        "- **Tuned Model Hyperparameters:**\n",
        "  - estimator__n_estimators': 200\n",
        "  - estimator__min_samples_split': 2\n",
        "  - estimator__min_samples_leaf': 1\n",
        "  - estimator__max_depth': 15\n",
        "  - estimator__bootstrap': True\n",
        "\n",
        "- **Cross-Validation Results (Tuned Model):**\n",
        "  - theoretical_capacity_grav:\n",
        "    - Mean R²: 0.8114 (Std Dev: 0.0472)\n",
        "    - Mean MAE: 22.7258 (Std Dev: 1.6363)\n",
        "    - Mean RMSE: 62.5175 (Std Dev: 15.0600)\n",
        "  - average_voltage:\n",
        "    - Mean R²: 0.7934 (Std Dev: 0.1878)\n",
        "    - Mean MAE: 0.4743 (Std Dev: 0.1984)\n",
        "    - Mean RMSE: 3.0983 (Std Dev: 4.0410)\n",
        "  - energy_above_hull:\n",
        "    - Mean R²: 0.4366 (Std Dev: 0.0128)\n",
        "    - Mean MAE: 0.0256 (Std Dev: 0.0004)\n",
        "    - Mean RMSE: 0.0340 (Std Dev: 0.0005)\n",
        "\n",
        "- **Test Set Evaluation (Tuned Model):**\n",
        "  - theoretical_capacity_grav:\n",
        "    - R²: 0.6406\n",
        "    - MAE: 19.0836\n",
        "    - RMSE: 42.8224\n",
        "  - average_voltage:\n",
        "    - R²: 0.8924\n",
        "    - MAE: 0.3814\n",
        "    - RMSE: 1.3002\n",
        "  - energy_above_hull:\n",
        "    - R²: 0.4496\n",
        "    - MAE: 0.0251\n",
        "    - RMSE: 0.0342\n",
        "\n",
        "### A.3 LightGBM Model\n",
        "\n",
        "- **Base Model Configuration:**\n",
        "  - objective: 'regression_l1'\n",
        "  - metric: 'mae'\n",
        "  - n_estimators: 300\n",
        "  - learning_rate: 0.1\n",
        "  - num_leaves: 31\n",
        "  - max_depth: -1\n",
        "  - min_child_samples: 20\n",
        "  - subsample: 0.8\n",
        "  - colsample_bytree: 0.8\n",
        "  - random_state: 9\n",
        "  - n_jobs: -1\n",
        "  - boosting_type: 'gbdt'\n",
        "\n",
        "- **Tuned Model Hyperparameters:**\n",
        "  - estimator__subsample: 0.7\n",
        "  - estimator__reg_lambda: 0\n",
        "  - estimator__reg_alpha: 0.01\n",
        "  - estimator__num_leaves: 20\n",
        "  - estimator__n_estimators: 300\n",
        "  - estimator__min_child_samples: 10\n",
        "  - estimator__max_depth: 10\n",
        "  - estimator__learning_rate: 0.1\n",
        "  - estimator__colsample_bytree: 0.7\n",
        "\n",
        "- **Cross-Validation Results (Tuned Model):**\n",
        "  - theoretical_capacity_grav:\n",
        "    - Mean R²: 0.7881 (Std Dev: 0.0630)\n",
        "    - Mean MAE: 23.6727 (Std Dev: 2.4109)\n",
        "    - Mean RMSE: 65.4221 (Std Dev: 12.6052)\n",
        "  - average_voltage:\n",
        "    - Mean R²: 0.5229 (Std Dev: 0.2852)\n",
        "    - Mean MAE: 0.5522 (Std Dev: 0.2745)\n",
        "    - Mean RMSE: 4.4684 (Std Dev: 4.9971)\n",
        "  - energy_above_hull:\n",
        "    - Mean R²: 0.4270 (Std Dev: 0.0202)\n",
        "    - Mean MAE: 0.0252 (Std Dev: 0.0005)\n",
        "    - Mean RMSE: 0.0343 (Std Dev: 0.0009)\n",
        "\n",
        "- **Test Set Evaluation (Tuned Model):**\n",
        "  - theoretical_capacity_grav:\n",
        "    - R²: 0.8093\n",
        "    - MAE: 18.0919\n",
        "    - RMSE: 31.1952\n",
        "  - average_voltage:\n",
        "    - R²: 0.5223\n",
        "    - MAE: 0.4794\n",
        "    - RMSE: 2.7391\n",
        "  - energy_above_hull:\n",
        "    - R²: 0.4505\n",
        "    - MAE: 0.0247\n",
        "    - RMSE: 0.0342\n",
        "\n",
        "## B. Data\n",
        "\n",
        "Information about the dataset used and processed in this notebook.\n",
        "\n",
        "- **Original Data Source:** `/content/drive/MyDrive/Colab Notebooks/Masters Research Project/Data/battery_materials_engineered_features_v2.csv`\n",
        "- **Shape of Engineered DataFrame:** (3445, 53)\n",
        "- **Defined Features (X):** 45 columns (list of columns provided in notebook)\n",
        "- **Defined Targets (Y):** 3 columns (`theoretical_capacity_grav`, `average_voltage`, `energy_above_hull`)\n",
        "- **Train-Test Split:** 80% Train (2756 samples), 20% Test (689 samples)\n",
        "- **Scaled Features:** 31 numerical columns were scaled using `StandardScaler`.\n",
        "- **Added Interaction Features:** 4 new features (`exp_band_gap`, `exp_energy_vol`, `theo_band_gap`, `theo_energy_vol`) added to scaled features.\n",
        "- **Final Scaled Features Shape:** (3445, 49)\n",
        "- **Saved Training/Test Data:**\n",
        "    - X_train_scaled.csv\n",
        "    - Y_train.csv\n",
        "    - X_test_scaled.csv\n",
        "    - Y_test.csv\n",
        "    - Saved to: `/content/drive/MyDrive/Colab Notebooks/Masters Research Project/Data/Training_Data`\n",
        "\n",
        "## C. Code\n",
        "\n",
        "Relevant code snippets and libraries used.\n",
        "\n",
        "- **Libraries Imported:** `pandas`, `numpy`, `os`, `sys`, `lightgbm`, `sklearn.model_selection`, `sklearn.preprocessing`, `sklearn.metrics`, `joblib`, `sklearn.multioutput`, `sklearn.ensemble`, `xgboost`, `random`\n",
        "- **Key Functions/Techniques:**\n",
        "    - `train_test_split`: For splitting data into training and testing sets.\n",
        "    - `StandardScaler`: For standardizing numerical features.\n",
        "    - `MultiOutputRegressor`: For handling multi-target regression with base estimators (XGBoost, RandomForest, LightGBM).\n",
        "    - `RandomizedSearchCV`: For hyperparameter tuning.\n",
        "    - `r2_score`, `mean_absolute_error`, `mean_squared_error`: For evaluating model performance.\n",
        "    - `joblib.dump`: For saving trained models and the scaler.\n",
        "    - Addition of interaction features through multiplication of existing columns.\n",
        "\n",
        "## D. Saved Files\n",
        "\n",
        "List of files saved during the execution of this notebook.\n",
        "\n",
        "- **Model Files:**\n",
        "    - `/content/drive/MyDrive/Colab Notebooks/Masters Research Project/Model/tuned_xgboost_multi_output_model_v2.joblib`\n",
        "    - `/content/drive/MyDrive/Colab Notebooks/Masters Research Project/Model/scaler_X_v2.joblib`\n",
        "    - `/content/drive/MyDrive/Colab Notebooks/Masters Research Project/Model/feature_column_names_v2.joblib`\n",
        "    - `/content/drive/MyDrive/Colab Notebooks/Masters Research Project/Model/tuned_random_forest_multi_output_model_v2.joblib`\n",
        "    - `/content/drive/MyDrive/Colab Notebooks/Masters Research Project/Model/tuned_lightgbm_multi_output_model_v2.joblib`\n",
        "\n",
        "- **Training/Test Data Files:**\n",
        "    - `/content/drive/MyDrive/Colab Notebooks/Masters Research Project/Data/Training_Data/X_train_scaled.csv`\n",
        "    - `/content/drive/MyDrive/Colab Notebooks/Masters Research Project/Data/Training_Data/Y_train.csv`\n",
        "    - `/content/drive/MyDrive/Colab Notebooks/Masters Research Project/Data/Training_Data/X_test_scaled.csv`\n",
        "    - `/content/drive/MyDrive/Colab Notebooks/Masters Research Project/Data/Training_Data/Y_test.csv`"
      ]
    }
  ]
}